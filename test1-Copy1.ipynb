{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, auc,roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import ttest_ind\n",
    "import time\n",
    "from datetime import date\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn import svm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['last_review', 'host_since'])\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = list(df.columns[1:24])\n",
    "df.columns.values.tolist()\n",
    "feat = [0,4,7,8,18,10,5,9,11,12,15]\n",
    "sort= [5,9]\n",
    "nah = [10,11,12,15]\n",
    "feature_list = []\n",
    "for i in feat:\n",
    "    feature_list += [features_mean[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is_business_travel_ready'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encodeval(dataframe,features_mean,col):\n",
    "    dataF = dataframe.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(dataF[features_mean[col]])\n",
    "    dataF[features_mean[col]] = le.transform(dataF[features_mean[col]])\n",
    "    return dataF\n",
    "\n",
    "df1['last_review'] = df['last_review'].dt.month\n",
    "df['host_since'] = pd.to_datetime(df['host_since']).dt.date\n",
    "df1['host_since'] = -(df['host_since']- date(2020, 10, 5)).dt.days\n",
    "df1 = encodeval(df1,features_mean,0)\n",
    "df1 = encodeval(df1,features_mean,1)\n",
    "df1 = encodeval(df1,features_mean,9)\n",
    "df1 = encodeval(df1,features_mean,13)\n",
    "df1 = encodeval(df1,features_mean,18)\n",
    "df1 = encodeval(df1,features_mean,19)\n",
    "df1 = encodeval(df1,features_mean,20)\n",
    "df1 = encodeval(df1,features_mean,21)\n",
    "df1 = encodeval(df1,features_mean,22)\n",
    "features_mean.pop(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.loc[:,features_mean]\n",
    "#X = (X.iloc[:,0:] - X.iloc[:,0:].mean()) / X.iloc[:,0:].std() #standardise\n",
    "y = df1.loc[:, 'price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:11:14] 6486x22 matrix with 108244 entries loaded from dtrain.svm\n",
      "[18:11:14] 3195x22 matrix with 53344 entries loaded from dtest.svm\n"
     ]
    }
   ],
   "source": [
    "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
    "dump_svmlight_file(X_val, y_val, 'dtest.svm', zero_based=True)\n",
    "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
    "dtest_svm = xgb.DMatrix('dtest.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 5}  # the number of classes that exist in this datset\n",
    "num_round = 100  # the number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5415679957137921\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_val, best_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5320813771517997\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, best_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.415962441314554"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# creating tensor from targets_df \n",
    "X_traintorch = torch.tensor(X_train.values)\n",
    "y_traintorch = torch.tensor(y_train.values)\n",
    "\n",
    "X_valtorch = torch.tensor(X_val.values)\n",
    "y_valtorch = torch.tensor(y_val.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_features, num_class):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(num_features, num_features//2)\n",
    "        self.layer_out = nn.Linear(num_features//2, num_class) \n",
    "        \n",
    "        \n",
    "        #self.lr = nn.LeakyReLU()\n",
    "        self.bn = nn.BatchNorm1d(num_features//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn(x)\n",
    "        #x = self.lr(x)        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet(22,5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.001 acc: 26.889\n",
      "[2] loss: 0.002 acc: 32.424\n",
      "[3] loss: 0.007 acc: 25.270\n",
      "[4] loss: 0.005 acc: 27.444\n",
      "[5] loss: 0.002 acc: 33.565\n",
      "[6] loss: 0.003 acc: 33.703\n",
      "[7] loss: 0.002 acc: 31.930\n",
      "[8] loss: 0.001 acc: 25.162\n",
      "[9] loss: 0.002 acc: 32.593\n",
      "[10] loss: 0.002 acc: 33.287\n",
      "[11] loss: 0.001 acc: 28.184\n",
      "[12] loss: 0.001 acc: 29.911\n",
      "[13] loss: 0.001 acc: 33.919\n",
      "[14] loss: 0.001 acc: 33.380\n",
      "[15] loss: 0.001 acc: 35.045\n",
      "[16] loss: 0.001 acc: 34.258\n",
      "[17] loss: 0.001 acc: 33.071\n",
      "[18] loss: 0.001 acc: 32.100\n",
      "[19] loss: 0.001 acc: 29.433\n",
      "[20] loss: 0.001 acc: 32.455\n",
      "[21] loss: 0.001 acc: 28.014\n",
      "[22] loss: 0.001 acc: 28.338\n",
      "[23] loss: 0.001 acc: 34.382\n",
      "[24] loss: 0.001 acc: 38.282\n",
      "[25] loss: 0.001 acc: 38.791\n",
      "[26] loss: 0.001 acc: 33.179\n",
      "[27] loss: 0.001 acc: 29.972\n",
      "[28] loss: 0.001 acc: 31.915\n",
      "[29] loss: 0.001 acc: 38.637\n",
      "[30] loss: 0.001 acc: 38.529\n",
      "[31] loss: 0.001 acc: 38.683\n",
      "[32] loss: 0.001 acc: 39.130\n",
      "[33] loss: 0.001 acc: 33.642\n",
      "[34] loss: 0.001 acc: 32.809\n",
      "[35] loss: 0.001 acc: 36.895\n",
      "[36] loss: 0.001 acc: 39.994\n",
      "[37] loss: 0.001 acc: 38.144\n",
      "[38] loss: 0.001 acc: 39.223\n",
      "[39] loss: 0.001 acc: 39.439\n",
      "[40] loss: 0.001 acc: 39.809\n",
      "[41] loss: 0.001 acc: 38.313\n",
      "[42] loss: 0.001 acc: 38.637\n",
      "[43] loss: 0.001 acc: 39.223\n",
      "[44] loss: 0.001 acc: 40.271\n",
      "[45] loss: 0.001 acc: 41.875\n",
      "[46] loss: 0.001 acc: 41.320\n",
      "[47] loss: 0.001 acc: 40.718\n",
      "[48] loss: 0.001 acc: 38.776\n",
      "[49] loss: 0.001 acc: 40.179\n",
      "[50] loss: 0.001 acc: 40.796\n",
      "[51] loss: 0.001 acc: 41.859\n",
      "[52] loss: 0.001 acc: 41.119\n",
      "[53] loss: 0.001 acc: 41.212\n",
      "[54] loss: 0.001 acc: 41.659\n",
      "[55] loss: 0.001 acc: 40.410\n",
      "[56] loss: 0.001 acc: 40.641\n",
      "[57] loss: 0.001 acc: 41.196\n",
      "[58] loss: 0.001 acc: 42.075\n",
      "[59] loss: 0.001 acc: 41.674\n",
      "[60] loss: 0.001 acc: 42.137\n",
      "[61] loss: 0.001 acc: 41.921\n",
      "[62] loss: 0.001 acc: 41.582\n",
      "[63] loss: 0.001 acc: 42.029\n",
      "[64] loss: 0.001 acc: 41.397\n",
      "[65] loss: 0.001 acc: 41.875\n",
      "[66] loss: 0.001 acc: 42.137\n",
      "[67] loss: 0.001 acc: 41.890\n",
      "[68] loss: 0.001 acc: 41.474\n",
      "[69] loss: 0.001 acc: 41.551\n",
      "[70] loss: 0.001 acc: 41.829\n",
      "[71] loss: 0.001 acc: 41.813\n",
      "[72] loss: 0.001 acc: 42.121\n",
      "[73] loss: 0.001 acc: 41.644\n",
      "[74] loss: 0.001 acc: 41.721\n",
      "[75] loss: 0.001 acc: 41.551\n",
      "[76] loss: 0.001 acc: 41.751\n",
      "[77] loss: 0.001 acc: 41.952\n",
      "[78] loss: 0.001 acc: 42.245\n",
      "[79] loss: 0.001 acc: 42.060\n",
      "[80] loss: 0.001 acc: 42.106\n",
      "[81] loss: 0.001 acc: 41.983\n",
      "[82] loss: 0.001 acc: 41.921\n",
      "[83] loss: 0.001 acc: 41.813\n",
      "[84] loss: 0.001 acc: 42.337\n",
      "[85] loss: 0.001 acc: 42.445\n",
      "[86] loss: 0.001 acc: 42.399\n",
      "[87] loss: 0.001 acc: 42.183\n",
      "[88] loss: 0.001 acc: 42.384\n",
      "[89] loss: 0.001 acc: 42.476\n",
      "[90] loss: 0.001 acc: 42.368\n",
      "[91] loss: 0.001 acc: 42.168\n",
      "[92] loss: 0.001 acc: 42.245\n",
      "[93] loss: 0.001 acc: 42.291\n",
      "[94] loss: 0.001 acc: 42.368\n",
      "[95] loss: 0.001 acc: 42.492\n",
      "[96] loss: 0.001 acc: 42.800\n",
      "[97] loss: 0.001 acc: 42.646\n",
      "[98] loss: 0.001 acc: 42.414\n",
      "[99] loss: 0.001 acc: 42.399\n",
      "[100] loss: 0.001 acc: 42.569\n",
      "[101] loss: 0.001 acc: 42.892\n",
      "[102] loss: 0.001 acc: 42.877\n",
      "[103] loss: 0.001 acc: 42.985\n",
      "[104] loss: 0.001 acc: 43.016\n",
      "[105] loss: 0.001 acc: 42.800\n",
      "[106] loss: 0.001 acc: 42.862\n",
      "[107] loss: 0.001 acc: 42.939\n",
      "[108] loss: 0.001 acc: 42.862\n",
      "[109] loss: 0.001 acc: 42.985\n",
      "[110] loss: 0.001 acc: 43.293\n",
      "[111] loss: 0.001 acc: 43.185\n",
      "[112] loss: 0.001 acc: 43.077\n",
      "[113] loss: 0.001 acc: 43.154\n",
      "[114] loss: 0.001 acc: 43.262\n",
      "[115] loss: 0.001 acc: 43.324\n",
      "[116] loss: 0.001 acc: 43.293\n",
      "[117] loss: 0.001 acc: 43.278\n",
      "[118] loss: 0.001 acc: 43.309\n",
      "[119] loss: 0.001 acc: 43.340\n",
      "[120] loss: 0.001 acc: 43.386\n",
      "[121] loss: 0.001 acc: 43.432\n",
      "[122] loss: 0.001 acc: 43.648\n",
      "[123] loss: 0.001 acc: 43.648\n",
      "[124] loss: 0.001 acc: 43.771\n",
      "[125] loss: 0.001 acc: 43.787\n",
      "[126] loss: 0.001 acc: 43.987\n",
      "[127] loss: 0.001 acc: 44.002\n",
      "[128] loss: 0.001 acc: 43.972\n",
      "[129] loss: 0.001 acc: 44.018\n",
      "[130] loss: 0.001 acc: 44.049\n",
      "[131] loss: 0.001 acc: 44.280\n",
      "[132] loss: 0.001 acc: 44.311\n",
      "[133] loss: 0.001 acc: 44.403\n",
      "[134] loss: 0.001 acc: 44.110\n",
      "[135] loss: 0.001 acc: 44.588\n",
      "[136] loss: 0.001 acc: 44.172\n",
      "[137] loss: 0.001 acc: 44.511\n",
      "[138] loss: 0.001 acc: 43.679\n",
      "[139] loss: 0.001 acc: 39.608\n",
      "[140] loss: 0.001 acc: 41.844\n",
      "[141] loss: 0.001 acc: 42.307\n",
      "[142] loss: 0.001 acc: 37.666\n",
      "[143] loss: 0.001 acc: 41.119\n",
      "[144] loss: 0.001 acc: 42.538\n",
      "[145] loss: 0.001 acc: 36.957\n",
      "[146] loss: 0.001 acc: 45.251\n",
      "[147] loss: 0.001 acc: 39.578\n",
      "[148] loss: 0.001 acc: 43.632\n",
      "[149] loss: 0.001 acc: 37.573\n",
      "[150] loss: 0.001 acc: 43.987\n",
      "[151] loss: 0.001 acc: 41.890\n",
      "[152] loss: 0.001 acc: 43.509\n",
      "[153] loss: 0.001 acc: 40.333\n",
      "[154] loss: 0.001 acc: 42.892\n",
      "[155] loss: 0.001 acc: 43.340\n",
      "[156] loss: 0.001 acc: 43.463\n",
      "[157] loss: 0.001 acc: 44.342\n",
      "[158] loss: 0.001 acc: 42.939\n",
      "[159] loss: 0.001 acc: 43.401\n",
      "[160] loss: 0.001 acc: 44.450\n",
      "[161] loss: 0.001 acc: 42.553\n",
      "[162] loss: 0.001 acc: 45.143\n",
      "[163] loss: 0.001 acc: 44.527\n",
      "[164] loss: 0.001 acc: 44.820\n",
      "[165] loss: 0.001 acc: 44.434\n",
      "[166] loss: 0.001 acc: 43.525\n",
      "[167] loss: 0.001 acc: 44.588\n",
      "[168] loss: 0.001 acc: 45.405\n",
      "[169] loss: 0.001 acc: 45.498\n",
      "[170] loss: 0.001 acc: 43.509\n",
      "[171] loss: 0.001 acc: 43.910\n",
      "[172] loss: 0.001 acc: 45.375\n",
      "[173] loss: 0.001 acc: 46.146\n",
      "[174] loss: 0.001 acc: 44.403\n",
      "[175] loss: 0.001 acc: 44.850\n",
      "[176] loss: 0.001 acc: 45.066\n",
      "[177] loss: 0.001 acc: 45.575\n",
      "[178] loss: 0.001 acc: 44.681\n",
      "[179] loss: 0.001 acc: 44.928\n",
      "[180] loss: 0.001 acc: 45.591\n",
      "[181] loss: 0.001 acc: 46.207\n",
      "[182] loss: 0.001 acc: 46.284\n",
      "[183] loss: 0.001 acc: 45.652\n",
      "[184] loss: 0.001 acc: 44.835\n",
      "[185] loss: 0.001 acc: 46.300\n",
      "[186] loss: 0.001 acc: 46.438\n",
      "[187] loss: 0.001 acc: 45.991\n",
      "[188] loss: 0.001 acc: 45.529\n",
      "[189] loss: 0.001 acc: 45.282\n",
      "[190] loss: 0.001 acc: 45.837\n",
      "[191] loss: 0.001 acc: 46.408\n",
      "[192] loss: 0.001 acc: 45.606\n",
      "[193] loss: 0.001 acc: 45.621\n",
      "[194] loss: 0.001 acc: 45.683\n",
      "[195] loss: 0.001 acc: 46.654\n",
      "[196] loss: 0.001 acc: 46.531\n",
      "[197] loss: 0.001 acc: 46.238\n",
      "[198] loss: 0.001 acc: 46.623\n",
      "[199] loss: 0.001 acc: 46.762\n",
      "[200] loss: 0.001 acc: 46.639\n",
      "[201] loss: 0.001 acc: 46.346\n",
      "[202] loss: 0.001 acc: 46.485\n",
      "[203] loss: 0.001 acc: 46.593\n",
      "[204] loss: 0.001 acc: 46.392\n",
      "[205] loss: 0.001 acc: 46.546\n",
      "[206] loss: 0.001 acc: 46.670\n",
      "[207] loss: 0.001 acc: 46.269\n",
      "[208] loss: 0.001 acc: 46.978\n",
      "[209] loss: 0.001 acc: 47.163\n",
      "[210] loss: 0.001 acc: 46.408\n",
      "[211] loss: 0.001 acc: 46.115\n",
      "[212] loss: 0.001 acc: 47.256\n",
      "[213] loss: 0.001 acc: 46.839\n",
      "[214] loss: 0.001 acc: 46.500\n",
      "[215] loss: 0.001 acc: 47.086\n",
      "[216] loss: 0.001 acc: 46.639\n",
      "[217] loss: 0.001 acc: 46.161\n",
      "[218] loss: 0.001 acc: 46.932\n",
      "[219] loss: 0.001 acc: 47.256\n",
      "[220] loss: 0.001 acc: 46.454\n",
      "[221] loss: 0.001 acc: 45.714\n",
      "[222] loss: 0.001 acc: 47.286\n",
      "[223] loss: 0.001 acc: 46.192\n",
      "[224] loss: 0.001 acc: 44.758\n",
      "[225] loss: 0.001 acc: 47.302\n",
      "[226] loss: 0.001 acc: 47.194\n",
      "[227] loss: 0.001 acc: 46.022\n",
      "[228] loss: 0.001 acc: 45.344\n",
      "[229] loss: 0.001 acc: 47.071\n",
      "[230] loss: 0.001 acc: 47.086\n",
      "[231] loss: 0.001 acc: 45.637\n",
      "[232] loss: 0.001 acc: 47.209\n",
      "[233] loss: 0.001 acc: 47.040\n",
      "[234] loss: 0.001 acc: 45.776\n",
      "[235] loss: 0.001 acc: 47.148\n",
      "[236] loss: 0.001 acc: 47.317\n",
      "[237] loss: 0.001 acc: 45.837\n",
      "[238] loss: 0.001 acc: 46.115\n",
      "[239] loss: 0.001 acc: 46.531\n",
      "[240] loss: 0.001 acc: 46.284\n",
      "[241] loss: 0.001 acc: 46.238\n",
      "[242] loss: 0.001 acc: 47.117\n",
      "[243] loss: 0.001 acc: 46.793\n",
      "[244] loss: 0.001 acc: 44.773\n",
      "[245] loss: 0.001 acc: 46.978\n",
      "[246] loss: 0.001 acc: 47.009\n",
      "[247] loss: 0.001 acc: 45.868\n",
      "[248] loss: 0.001 acc: 46.438\n",
      "[249] loss: 0.001 acc: 46.608\n",
      "[250] loss: 0.001 acc: 45.714\n",
      "[251] loss: 0.001 acc: 45.883\n",
      "[252] loss: 0.001 acc: 47.364\n",
      "[253] loss: 0.001 acc: 46.562\n",
      "[254] loss: 0.001 acc: 46.361\n",
      "[255] loss: 0.001 acc: 45.637\n",
      "[256] loss: 0.001 acc: 46.207\n",
      "[257] loss: 0.001 acc: 47.410\n",
      "[258] loss: 0.001 acc: 45.883\n",
      "[259] loss: 0.001 acc: 47.225\n",
      "[260] loss: 0.001 acc: 46.500\n",
      "[261] loss: 0.001 acc: 45.776\n",
      "[262] loss: 0.001 acc: 47.163\n",
      "[263] loss: 0.001 acc: 47.672\n",
      "[264] loss: 0.001 acc: 46.485\n",
      "[265] loss: 0.001 acc: 46.099\n",
      "[266] loss: 0.001 acc: 46.685\n",
      "[267] loss: 0.001 acc: 47.009\n",
      "[268] loss: 0.001 acc: 46.947\n",
      "[269] loss: 0.001 acc: 47.749\n",
      "[270] loss: 0.001 acc: 46.809\n",
      "[271] loss: 0.001 acc: 46.623\n",
      "[272] loss: 0.001 acc: 46.762\n",
      "[273] loss: 0.001 acc: 47.764\n",
      "[274] loss: 0.001 acc: 46.022\n",
      "[275] loss: 0.001 acc: 46.300\n",
      "[276] loss: 0.001 acc: 47.641\n",
      "[277] loss: 0.001 acc: 47.040\n",
      "[278] loss: 0.001 acc: 46.886\n",
      "[279] loss: 0.001 acc: 46.870\n",
      "[280] loss: 0.001 acc: 45.328\n",
      "[281] loss: 0.001 acc: 47.117\n",
      "[282] loss: 0.001 acc: 47.071\n",
      "[283] loss: 0.001 acc: 46.223\n",
      "[284] loss: 0.001 acc: 46.068\n",
      "[285] loss: 0.001 acc: 46.423\n",
      "[286] loss: 0.001 acc: 46.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287] loss: 0.001 acc: 46.963\n",
      "[288] loss: 0.001 acc: 47.533\n",
      "[289] loss: 0.001 acc: 47.040\n",
      "[290] loss: 0.001 acc: 45.606\n",
      "[291] loss: 0.001 acc: 46.778\n",
      "[292] loss: 0.001 acc: 46.377\n",
      "[293] loss: 0.001 acc: 47.641\n",
      "[294] loss: 0.001 acc: 45.359\n",
      "[295] loss: 0.001 acc: 47.209\n",
      "[296] loss: 0.001 acc: 46.392\n",
      "[297] loss: 0.001 acc: 47.348\n",
      "[298] loss: 0.001 acc: 46.639\n",
      "[299] loss: 0.001 acc: 47.518\n",
      "[300] loss: 0.001 acc: 46.099\n",
      "[301] loss: 0.001 acc: 46.207\n",
      "[302] loss: 0.001 acc: 47.471\n",
      "[303] loss: 0.001 acc: 47.441\n",
      "[304] loss: 0.001 acc: 47.749\n",
      "[305] loss: 0.001 acc: 46.068\n",
      "[306] loss: 0.001 acc: 47.626\n",
      "[307] loss: 0.001 acc: 47.595\n",
      "[308] loss: 0.001 acc: 47.811\n",
      "[309] loss: 0.001 acc: 46.916\n",
      "[310] loss: 0.001 acc: 47.286\n",
      "[311] loss: 0.001 acc: 47.980\n",
      "[312] loss: 0.001 acc: 46.670\n",
      "[313] loss: 0.001 acc: 47.271\n",
      "[314] loss: 0.001 acc: 47.749\n",
      "[315] loss: 0.001 acc: 45.822\n",
      "[316] loss: 0.001 acc: 46.408\n",
      "[317] loss: 0.001 acc: 46.809\n",
      "[318] loss: 0.001 acc: 46.469\n",
      "[319] loss: 0.001 acc: 46.916\n",
      "[320] loss: 0.001 acc: 46.716\n",
      "[321] loss: 0.001 acc: 44.403\n",
      "[322] loss: 0.001 acc: 46.670\n",
      "[323] loss: 0.001 acc: 46.855\n",
      "[324] loss: 0.001 acc: 45.575\n",
      "[325] loss: 0.001 acc: 46.392\n",
      "[326] loss: 0.001 acc: 47.394\n",
      "[327] loss: 0.001 acc: 47.024\n",
      "[328] loss: 0.001 acc: 46.284\n",
      "[329] loss: 0.001 acc: 47.564\n",
      "[330] loss: 0.001 acc: 47.980\n",
      "[331] loss: 0.001 acc: 46.886\n",
      "[332] loss: 0.001 acc: 45.328\n",
      "[333] loss: 0.001 acc: 48.242\n",
      "[334] loss: 0.001 acc: 47.271\n",
      "[335] loss: 0.001 acc: 47.934\n",
      "[336] loss: 0.001 acc: 46.639\n",
      "[337] loss: 0.001 acc: 46.778\n",
      "[338] loss: 0.001 acc: 47.872\n",
      "[339] loss: 0.001 acc: 47.487\n",
      "[340] loss: 0.001 acc: 46.994\n",
      "[341] loss: 0.001 acc: 46.716\n",
      "[342] loss: 0.001 acc: 46.577\n",
      "[343] loss: 0.001 acc: 47.764\n",
      "[344] loss: 0.001 acc: 47.101\n",
      "[345] loss: 0.001 acc: 48.104\n",
      "[346] loss: 0.001 acc: 47.040\n",
      "[347] loss: 0.001 acc: 47.379\n",
      "[348] loss: 0.001 acc: 47.780\n",
      "[349] loss: 0.001 acc: 47.687\n",
      "[350] loss: 0.001 acc: 47.764\n",
      "[351] loss: 0.001 acc: 47.209\n",
      "[352] loss: 0.001 acc: 47.163\n",
      "[353] loss: 0.001 acc: 47.549\n",
      "[354] loss: 0.001 acc: 48.181\n",
      "[355] loss: 0.001 acc: 47.317\n",
      "[356] loss: 0.001 acc: 47.610\n",
      "[357] loss: 0.001 acc: 48.273\n",
      "[358] loss: 0.001 acc: 48.027\n",
      "[359] loss: 0.001 acc: 47.533\n",
      "[360] loss: 0.001 acc: 47.965\n",
      "[361] loss: 0.001 acc: 47.441\n",
      "[362] loss: 0.001 acc: 46.855\n",
      "[363] loss: 0.001 acc: 48.520\n",
      "[364] loss: 0.001 acc: 48.397\n",
      "[365] loss: 0.001 acc: 47.441\n",
      "[366] loss: 0.001 acc: 47.949\n",
      "[367] loss: 0.001 acc: 47.364\n",
      "[368] loss: 0.001 acc: 47.317\n",
      "[369] loss: 0.001 acc: 48.689\n",
      "[370] loss: 0.001 acc: 48.535\n",
      "[371] loss: 0.001 acc: 47.795\n",
      "[372] loss: 0.001 acc: 48.242\n",
      "[373] loss: 0.001 acc: 48.628\n",
      "[374] loss: 0.001 acc: 48.119\n",
      "[375] loss: 0.001 acc: 48.489\n",
      "[376] loss: 0.001 acc: 48.566\n",
      "[377] loss: 0.001 acc: 48.027\n",
      "[378] loss: 0.001 acc: 48.751\n",
      "[379] loss: 0.001 acc: 47.996\n",
      "[380] loss: 0.001 acc: 47.949\n",
      "[381] loss: 0.001 acc: 48.859\n",
      "[382] loss: 0.001 acc: 48.813\n",
      "[383] loss: 0.001 acc: 48.027\n",
      "[384] loss: 0.001 acc: 48.628\n",
      "[385] loss: 0.001 acc: 48.381\n",
      "[386] loss: 0.001 acc: 48.258\n",
      "[387] loss: 0.001 acc: 48.643\n",
      "[388] loss: 0.001 acc: 48.844\n",
      "[389] loss: 0.001 acc: 48.381\n",
      "[390] loss: 0.001 acc: 48.458\n",
      "[391] loss: 0.001 acc: 46.947\n",
      "[392] loss: 0.001 acc: 46.670\n",
      "[393] loss: 0.001 acc: 44.249\n",
      "[394] loss: 0.001 acc: 44.650\n",
      "[395] loss: 0.001 acc: 46.115\n",
      "[396] loss: 0.001 acc: 44.234\n",
      "[397] loss: 0.001 acc: 44.033\n",
      "[398] loss: 0.001 acc: 45.544\n",
      "[399] loss: 0.001 acc: 45.976\n",
      "[400] loss: 0.001 acc: 43.956\n",
      "[401] loss: 0.001 acc: 45.190\n",
      "[402] loss: 0.001 acc: 42.584\n",
      "[403] loss: 0.001 acc: 43.571\n",
      "[404] loss: 0.001 acc: 41.582\n",
      "[405] loss: 0.001 acc: 42.199\n",
      "[406] loss: 0.001 acc: 43.093\n",
      "[407] loss: 0.001 acc: 44.110\n",
      "[408] loss: 0.001 acc: 44.126\n",
      "[409] loss: 0.001 acc: 44.573\n",
      "[410] loss: 0.001 acc: 44.897\n",
      "[411] loss: 0.001 acc: 43.879\n",
      "[412] loss: 0.001 acc: 44.789\n",
      "[413] loss: 0.001 acc: 44.650\n",
      "[414] loss: 0.001 acc: 46.022\n",
      "[415] loss: 0.001 acc: 45.914\n",
      "[416] loss: 0.001 acc: 46.315\n",
      "[417] loss: 0.001 acc: 46.654\n",
      "[418] loss: 0.001 acc: 46.161\n",
      "[419] loss: 0.001 acc: 46.994\n",
      "[420] loss: 0.001 acc: 46.315\n",
      "[421] loss: 0.001 acc: 44.866\n",
      "[422] loss: 0.001 acc: 45.637\n",
      "[423] loss: 0.001 acc: 46.947\n",
      "[424] loss: 0.001 acc: 46.608\n",
      "[425] loss: 0.001 acc: 47.101\n",
      "[426] loss: 0.001 acc: 47.256\n",
      "[427] loss: 0.001 acc: 46.315\n",
      "[428] loss: 0.001 acc: 47.811\n",
      "[429] loss: 0.001 acc: 46.269\n",
      "[430] loss: 0.001 acc: 46.392\n",
      "[431] loss: 0.001 acc: 46.084\n",
      "[432] loss: 0.001 acc: 47.656\n",
      "[433] loss: 0.001 acc: 47.132\n",
      "[434] loss: 0.001 acc: 47.071\n",
      "[435] loss: 0.001 acc: 46.207\n",
      "[436] loss: 0.001 acc: 47.194\n",
      "[437] loss: 0.001 acc: 47.811\n",
      "[438] loss: 0.001 acc: 47.410\n",
      "[439] loss: 0.001 acc: 45.560\n",
      "[440] loss: 0.001 acc: 46.207\n",
      "[441] loss: 0.001 acc: 46.963\n",
      "[442] loss: 0.001 acc: 47.579\n",
      "[443] loss: 0.001 acc: 45.945\n",
      "[444] loss: 0.001 acc: 47.179\n",
      "[445] loss: 0.001 acc: 47.487\n",
      "[446] loss: 0.001 acc: 47.209\n",
      "[447] loss: 0.001 acc: 46.068\n",
      "[448] loss: 0.001 acc: 45.328\n",
      "[449] loss: 0.001 acc: 47.780\n",
      "[450] loss: 0.001 acc: 47.225\n",
      "[451] loss: 0.001 acc: 47.364\n",
      "[452] loss: 0.001 acc: 47.009\n",
      "[453] loss: 0.001 acc: 47.456\n",
      "[454] loss: 0.001 acc: 47.687\n",
      "[455] loss: 0.001 acc: 47.225\n",
      "[456] loss: 0.001 acc: 46.870\n",
      "[457] loss: 0.001 acc: 48.088\n",
      "[458] loss: 0.001 acc: 47.703\n",
      "[459] loss: 0.001 acc: 47.379\n",
      "[460] loss: 0.001 acc: 47.610\n",
      "[461] loss: 0.001 acc: 47.919\n",
      "[462] loss: 0.001 acc: 47.965\n",
      "[463] loss: 0.001 acc: 47.441\n",
      "[464] loss: 0.001 acc: 48.104\n",
      "[465] loss: 0.001 acc: 47.471\n",
      "[466] loss: 0.001 acc: 47.888\n",
      "[467] loss: 0.001 acc: 47.903\n",
      "[468] loss: 0.001 acc: 48.119\n",
      "[469] loss: 0.001 acc: 47.903\n",
      "[470] loss: 0.001 acc: 47.857\n",
      "[471] loss: 0.001 acc: 48.289\n",
      "[472] loss: 0.001 acc: 48.119\n",
      "[473] loss: 0.001 acc: 47.502\n",
      "[474] loss: 0.001 acc: 47.348\n",
      "[475] loss: 0.001 acc: 48.242\n",
      "[476] loss: 0.001 acc: 47.502\n",
      "[477] loss: 0.001 acc: 47.764\n",
      "[478] loss: 0.001 acc: 48.212\n",
      "[479] loss: 0.001 acc: 48.104\n",
      "[480] loss: 0.001 acc: 47.903\n",
      "[481] loss: 0.001 acc: 48.073\n",
      "[482] loss: 0.001 acc: 47.626\n",
      "[483] loss: 0.001 acc: 47.533\n",
      "[484] loss: 0.001 acc: 48.073\n",
      "[485] loss: 0.001 acc: 47.564\n",
      "[486] loss: 0.001 acc: 47.934\n",
      "[487] loss: 0.001 acc: 48.165\n",
      "[488] loss: 0.001 acc: 47.888\n",
      "[489] loss: 0.001 acc: 47.857\n",
      "[490] loss: 0.001 acc: 48.258\n",
      "[491] loss: 0.001 acc: 47.826\n",
      "[492] loss: 0.001 acc: 48.628\n",
      "[493] loss: 0.001 acc: 48.674\n",
      "[494] loss: 0.001 acc: 48.566\n",
      "[495] loss: 0.001 acc: 48.659\n",
      "[496] loss: 0.001 acc: 48.844\n",
      "[497] loss: 0.001 acc: 48.844\n",
      "[498] loss: 0.001 acc: 48.566\n",
      "[499] loss: 0.001 acc: 48.967\n",
      "[500] loss: 0.001 acc: 49.322\n",
      "[501] loss: 0.001 acc: 48.427\n",
      "[502] loss: 0.001 acc: 49.090\n",
      "[503] loss: 0.001 acc: 48.782\n",
      "[504] loss: 0.001 acc: 49.060\n",
      "[505] loss: 0.001 acc: 48.612\n",
      "[506] loss: 0.001 acc: 48.335\n",
      "[507] loss: 0.001 acc: 48.952\n",
      "[508] loss: 0.001 acc: 49.676\n",
      "[509] loss: 0.001 acc: 48.397\n",
      "[510] loss: 0.001 acc: 48.073\n",
      "[511] loss: 0.001 acc: 49.090\n",
      "[512] loss: 0.001 acc: 48.474\n",
      "[513] loss: 0.001 acc: 49.121\n",
      "[514] loss: 0.001 acc: 48.597\n",
      "[515] loss: 0.001 acc: 48.335\n",
      "[516] loss: 0.001 acc: 48.474\n",
      "[517] loss: 0.001 acc: 49.106\n",
      "[518] loss: 0.001 acc: 47.949\n",
      "[519] loss: 0.001 acc: 48.659\n",
      "[520] loss: 0.001 acc: 48.705\n",
      "[521] loss: 0.001 acc: 47.980\n",
      "[522] loss: 0.001 acc: 48.427\n",
      "[523] loss: 0.001 acc: 48.597\n",
      "[524] loss: 0.001 acc: 48.227\n",
      "[525] loss: 0.001 acc: 48.504\n",
      "[526] loss: 0.001 acc: 48.844\n",
      "[527] loss: 0.001 acc: 48.273\n",
      "[528] loss: 0.001 acc: 48.705\n",
      "[529] loss: 0.001 acc: 48.967\n",
      "[530] loss: 0.001 acc: 48.689\n",
      "[531] loss: 0.001 acc: 48.797\n",
      "[532] loss: 0.001 acc: 48.520\n",
      "[533] loss: 0.001 acc: 48.936\n",
      "[534] loss: 0.001 acc: 48.689\n",
      "[535] loss: 0.001 acc: 48.905\n",
      "[536] loss: 0.001 acc: 48.104\n",
      "[537] loss: 0.001 acc: 47.117\n",
      "[538] loss: 0.001 acc: 45.621\n",
      "[539] loss: 0.001 acc: 40.950\n",
      "[540] loss: 0.001 acc: 43.047\n",
      "[541] loss: 0.001 acc: 31.514\n",
      "[542] loss: 0.003 acc: 33.549\n",
      "[543] loss: 0.007 acc: 25.270\n",
      "[544] loss: 0.002 acc: 27.397\n",
      "[545] loss: 0.007 acc: 33.457\n",
      "[546] loss: 0.017 acc: 25.301\n",
      "[547] loss: 0.004 acc: 36.448\n",
      "[548] loss: 0.006 acc: 38.807\n",
      "[549] loss: 0.005 acc: 22.541\n",
      "[550] loss: 0.010 acc: 26.179\n",
      "[551] loss: 0.004 acc: 27.521\n",
      "[552] loss: 0.009 acc: 32.902\n",
      "[553] loss: 0.008 acc: 26.041\n",
      "[554] loss: 0.004 acc: 25.840\n",
      "[555] loss: 0.009 acc: 25.763\n",
      "[556] loss: 0.004 acc: 27.829\n",
      "[557] loss: 0.007 acc: 33.457\n",
      "[558] loss: 0.005 acc: 39.377\n",
      "[559] loss: 0.003 acc: 24.345\n",
      "[560] loss: 0.008 acc: 25.270\n",
      "[561] loss: 0.002 acc: 33.673\n",
      "[562] loss: 0.003 acc: 33.010\n",
      "[563] loss: 0.004 acc: 37.974\n",
      "[564] loss: 0.004 acc: 15.880\n",
      "[565] loss: 0.003 acc: 28.122\n",
      "[566] loss: 0.003 acc: 30.142\n",
      "[567] loss: 0.004 acc: 29.479\n",
      "[568] loss: 0.005 acc: 38.976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[569] loss: 0.005 acc: 38.159\n",
      "[570] loss: 0.004 acc: 23.096\n",
      "[571] loss: 0.003 acc: 27.552\n",
      "[572] loss: 0.002 acc: 30.034\n",
      "[573] loss: 0.003 acc: 24.561\n",
      "[574] loss: 0.002 acc: 31.946\n",
      "[575] loss: 0.003 acc: 23.158\n",
      "[576] loss: 0.004 acc: 26.565\n",
      "[577] loss: 0.004 acc: 34.073\n",
      "[578] loss: 0.004 acc: 38.483\n",
      "[579] loss: 0.003 acc: 31.360\n",
      "[580] loss: 0.003 acc: 28.662\n",
      "[581] loss: 0.002 acc: 19.087\n",
      "[582] loss: 0.002 acc: 30.558\n",
      "[583] loss: 0.004 acc: 31.129\n",
      "[584] loss: 0.003 acc: 26.565\n",
      "[585] loss: 0.003 acc: 28.708\n",
      "[586] loss: 0.005 acc: 28.893\n",
      "[587] loss: 0.008 acc: 30.142\n",
      "[588] loss: 0.004 acc: 38.514\n",
      "[589] loss: 0.004 acc: 37.974\n",
      "[590] loss: 0.002 acc: 34.952\n",
      "[591] loss: 0.002 acc: 26.688\n",
      "[592] loss: 0.003 acc: 26.596\n",
      "[593] loss: 0.002 acc: 36.849\n",
      "[594] loss: 0.003 acc: 35.137\n",
      "[595] loss: 0.002 acc: 35.091\n",
      "[596] loss: 0.001 acc: 24.715\n",
      "[597] loss: 0.002 acc: 26.334\n",
      "[598] loss: 0.001 acc: 26.426\n",
      "[599] loss: 0.001 acc: 38.190\n",
      "[600] loss: 0.002 acc: 37.172\n",
      "[601] loss: 0.001 acc: 25.100\n",
      "[602] loss: 0.001 acc: 25.578\n",
      "[603] loss: 0.001 acc: 27.814\n",
      "[604] loss: 0.001 acc: 30.512\n",
      "[605] loss: 0.001 acc: 33.842\n",
      "[606] loss: 0.001 acc: 24.191\n",
      "[607] loss: 0.001 acc: 35.353\n",
      "[608] loss: 0.001 acc: 40.025\n",
      "[609] loss: 0.001 acc: 39.007\n",
      "[610] loss: 0.001 acc: 37.866\n",
      "[611] loss: 0.001 acc: 36.741\n",
      "[612] loss: 0.001 acc: 37.805\n",
      "[613] loss: 0.001 acc: 26.442\n",
      "[614] loss: 0.001 acc: 37.604\n",
      "[615] loss: 0.001 acc: 30.759\n",
      "[616] loss: 0.001 acc: 34.721\n",
      "[617] loss: 0.001 acc: 35.862\n",
      "[618] loss: 0.001 acc: 33.087\n",
      "[619] loss: 0.001 acc: 34.860\n",
      "[620] loss: 0.001 acc: 38.421\n",
      "[621] loss: 0.001 acc: 38.298\n",
      "[622] loss: 0.001 acc: 36.294\n",
      "[623] loss: 0.001 acc: 33.780\n",
      "[624] loss: 0.001 acc: 39.870\n",
      "[625] loss: 0.001 acc: 38.282\n",
      "[626] loss: 0.001 acc: 34.690\n",
      "[627] loss: 0.001 acc: 36.494\n",
      "[628] loss: 0.001 acc: 39.470\n",
      "[629] loss: 0.001 acc: 38.467\n",
      "[630] loss: 0.001 acc: 38.190\n",
      "[631] loss: 0.001 acc: 38.036\n",
      "[632] loss: 0.001 acc: 40.811\n",
      "[633] loss: 0.001 acc: 40.688\n",
      "[634] loss: 0.001 acc: 41.243\n",
      "[635] loss: 0.001 acc: 39.855\n",
      "[636] loss: 0.001 acc: 41.536\n",
      "[637] loss: 0.001 acc: 40.086\n",
      "[638] loss: 0.001 acc: 40.934\n",
      "[639] loss: 0.001 acc: 40.580\n",
      "[640] loss: 0.001 acc: 39.269\n",
      "[641] loss: 0.001 acc: 39.238\n",
      "[642] loss: 0.001 acc: 40.549\n",
      "[643] loss: 0.001 acc: 40.611\n",
      "[644] loss: 0.001 acc: 40.857\n",
      "[645] loss: 0.001 acc: 41.397\n",
      "[646] loss: 0.001 acc: 40.965\n",
      "[647] loss: 0.001 acc: 41.443\n",
      "[648] loss: 0.001 acc: 41.751\n",
      "[649] loss: 0.001 acc: 42.060\n",
      "[650] loss: 0.001 acc: 42.137\n",
      "[651] loss: 0.001 acc: 41.536\n",
      "[652] loss: 0.001 acc: 41.551\n",
      "[653] loss: 0.001 acc: 42.106\n",
      "[654] loss: 0.001 acc: 41.998\n",
      "[655] loss: 0.001 acc: 42.014\n",
      "[656] loss: 0.001 acc: 41.597\n",
      "[657] loss: 0.001 acc: 41.613\n",
      "[658] loss: 0.001 acc: 41.859\n",
      "[659] loss: 0.001 acc: 41.829\n",
      "[660] loss: 0.001 acc: 41.798\n",
      "[661] loss: 0.001 acc: 41.628\n",
      "[662] loss: 0.001 acc: 41.936\n",
      "[663] loss: 0.001 acc: 42.183\n",
      "[664] loss: 0.001 acc: 42.029\n",
      "[665] loss: 0.001 acc: 42.353\n",
      "[666] loss: 0.001 acc: 41.412\n",
      "[667] loss: 0.001 acc: 41.582\n",
      "[668] loss: 0.001 acc: 41.582\n",
      "[669] loss: 0.001 acc: 41.844\n",
      "[670] loss: 0.001 acc: 41.936\n",
      "[671] loss: 0.001 acc: 41.751\n",
      "[672] loss: 0.001 acc: 41.890\n",
      "[673] loss: 0.001 acc: 41.844\n",
      "[674] loss: 0.001 acc: 41.906\n",
      "[675] loss: 0.001 acc: 41.381\n",
      "[676] loss: 0.001 acc: 41.212\n",
      "[677] loss: 0.001 acc: 41.166\n",
      "[678] loss: 0.001 acc: 41.443\n",
      "[679] loss: 0.001 acc: 41.505\n",
      "[680] loss: 0.001 acc: 41.551\n",
      "[681] loss: 0.001 acc: 41.505\n",
      "[682] loss: 0.001 acc: 41.381\n",
      "[683] loss: 0.001 acc: 41.551\n",
      "[684] loss: 0.001 acc: 41.613\n",
      "[685] loss: 0.001 acc: 41.721\n",
      "[686] loss: 0.001 acc: 41.505\n",
      "[687] loss: 0.001 acc: 41.474\n",
      "[688] loss: 0.001 acc: 41.428\n",
      "[689] loss: 0.001 acc: 41.505\n",
      "[690] loss: 0.001 acc: 41.381\n",
      "[691] loss: 0.001 acc: 41.428\n",
      "[692] loss: 0.001 acc: 41.597\n",
      "[693] loss: 0.001 acc: 41.459\n",
      "[694] loss: 0.001 acc: 41.613\n",
      "[695] loss: 0.001 acc: 41.474\n",
      "[696] loss: 0.001 acc: 41.566\n",
      "[697] loss: 0.001 acc: 41.921\n",
      "[698] loss: 0.001 acc: 41.751\n",
      "[699] loss: 0.001 acc: 41.751\n",
      "[700] loss: 0.001 acc: 41.736\n",
      "[701] loss: 0.001 acc: 41.751\n",
      "[702] loss: 0.001 acc: 41.782\n",
      "[703] loss: 0.001 acc: 41.751\n",
      "[704] loss: 0.001 acc: 41.721\n",
      "[705] loss: 0.001 acc: 41.798\n",
      "[706] loss: 0.001 acc: 41.705\n",
      "[707] loss: 0.001 acc: 41.721\n",
      "[708] loss: 0.001 acc: 41.844\n",
      "[709] loss: 0.001 acc: 41.844\n",
      "[710] loss: 0.001 acc: 41.829\n",
      "[711] loss: 0.001 acc: 41.798\n",
      "[712] loss: 0.001 acc: 41.829\n",
      "[713] loss: 0.001 acc: 41.921\n",
      "[714] loss: 0.001 acc: 42.075\n",
      "[715] loss: 0.001 acc: 42.106\n",
      "[716] loss: 0.001 acc: 42.229\n",
      "[717] loss: 0.001 acc: 42.214\n",
      "[718] loss: 0.001 acc: 42.260\n",
      "[719] loss: 0.001 acc: 42.322\n",
      "[720] loss: 0.001 acc: 42.430\n",
      "[721] loss: 0.001 acc: 42.384\n",
      "[722] loss: 0.001 acc: 42.507\n",
      "[723] loss: 0.001 acc: 42.399\n",
      "[724] loss: 0.001 acc: 42.507\n",
      "[725] loss: 0.001 acc: 42.769\n",
      "[726] loss: 0.001 acc: 42.800\n",
      "[727] loss: 0.001 acc: 42.815\n",
      "[728] loss: 0.001 acc: 42.831\n",
      "[729] loss: 0.001 acc: 42.939\n",
      "[730] loss: 0.001 acc: 42.985\n",
      "[731] loss: 0.001 acc: 42.969\n",
      "[732] loss: 0.001 acc: 42.985\n",
      "[733] loss: 0.001 acc: 43.016\n",
      "[734] loss: 0.001 acc: 43.077\n",
      "[735] loss: 0.001 acc: 43.062\n",
      "[736] loss: 0.001 acc: 43.093\n",
      "[737] loss: 0.001 acc: 43.216\n",
      "[738] loss: 0.001 acc: 43.309\n",
      "[739] loss: 0.001 acc: 43.355\n",
      "[740] loss: 0.001 acc: 43.386\n",
      "[741] loss: 0.001 acc: 43.340\n",
      "[742] loss: 0.001 acc: 43.293\n",
      "[743] loss: 0.001 acc: 43.355\n",
      "[744] loss: 0.001 acc: 43.340\n",
      "[745] loss: 0.001 acc: 43.355\n",
      "[746] loss: 0.001 acc: 43.324\n",
      "[747] loss: 0.001 acc: 43.185\n",
      "[748] loss: 0.001 acc: 43.170\n",
      "[749] loss: 0.001 acc: 43.293\n",
      "[750] loss: 0.001 acc: 43.324\n",
      "[751] loss: 0.001 acc: 43.432\n",
      "[752] loss: 0.001 acc: 43.478\n",
      "[753] loss: 0.001 acc: 43.463\n",
      "[754] loss: 0.001 acc: 43.447\n",
      "[755] loss: 0.001 acc: 43.401\n",
      "[756] loss: 0.001 acc: 43.370\n",
      "[757] loss: 0.001 acc: 43.447\n",
      "[758] loss: 0.001 acc: 43.447\n",
      "[759] loss: 0.001 acc: 43.494\n",
      "[760] loss: 0.001 acc: 43.463\n",
      "[761] loss: 0.001 acc: 43.648\n",
      "[762] loss: 0.001 acc: 43.710\n",
      "[763] loss: 0.001 acc: 43.663\n",
      "[764] loss: 0.001 acc: 43.586\n",
      "[765] loss: 0.001 acc: 43.648\n",
      "[766] loss: 0.001 acc: 43.787\n",
      "[767] loss: 0.001 acc: 43.756\n",
      "[768] loss: 0.001 acc: 43.771\n",
      "[769] loss: 0.001 acc: 43.879\n",
      "[770] loss: 0.001 acc: 43.879\n",
      "[771] loss: 0.001 acc: 43.864\n",
      "[772] loss: 0.001 acc: 43.879\n",
      "[773] loss: 0.001 acc: 43.925\n",
      "[774] loss: 0.001 acc: 44.110\n",
      "[775] loss: 0.001 acc: 44.172\n",
      "[776] loss: 0.001 acc: 44.218\n",
      "[777] loss: 0.001 acc: 44.249\n",
      "[778] loss: 0.001 acc: 44.295\n",
      "[779] loss: 0.001 acc: 44.434\n",
      "[780] loss: 0.001 acc: 44.511\n",
      "[781] loss: 0.001 acc: 44.635\n",
      "[782] loss: 0.001 acc: 44.696\n",
      "[783] loss: 0.001 acc: 44.665\n",
      "[784] loss: 0.001 acc: 44.789\n",
      "[785] loss: 0.001 acc: 44.835\n",
      "[786] loss: 0.001 acc: 44.866\n",
      "[787] loss: 0.001 acc: 45.035\n",
      "[788] loss: 0.001 acc: 45.267\n",
      "[789] loss: 0.001 acc: 45.236\n",
      "[790] loss: 0.001 acc: 45.328\n",
      "[791] loss: 0.001 acc: 45.205\n",
      "[792] loss: 0.001 acc: 45.282\n",
      "[793] loss: 0.001 acc: 44.758\n",
      "[794] loss: 0.001 acc: 44.311\n",
      "[795] loss: 0.001 acc: 44.342\n",
      "[796] loss: 0.001 acc: 41.859\n",
      "[797] loss: 0.001 acc: 44.080\n",
      "[798] loss: 0.001 acc: 43.972\n",
      "[799] loss: 0.001 acc: 44.249\n",
      "[800] loss: 0.001 acc: 43.309\n",
      "[801] loss: 0.001 acc: 44.558\n",
      "[802] loss: 0.001 acc: 40.271\n",
      "[803] loss: 0.001 acc: 41.304\n",
      "[804] loss: 0.001 acc: 40.734\n",
      "[805] loss: 0.001 acc: 39.716\n",
      "[806] loss: 0.001 acc: 38.329\n",
      "[807] loss: 0.001 acc: 39.146\n",
      "[808] loss: 0.001 acc: 39.932\n",
      "[809] loss: 0.001 acc: 37.188\n",
      "[810] loss: 0.001 acc: 35.646\n",
      "[811] loss: 0.001 acc: 40.811\n",
      "[812] loss: 0.001 acc: 32.624\n",
      "[813] loss: 0.001 acc: 34.844\n",
      "[814] loss: 0.001 acc: 36.556\n",
      "[815] loss: 0.001 acc: 33.179\n",
      "[816] loss: 0.001 acc: 37.188\n",
      "[817] loss: 0.001 acc: 41.459\n",
      "[818] loss: 0.001 acc: 40.672\n",
      "[819] loss: 0.001 acc: 29.510\n",
      "[820] loss: 0.001 acc: 36.078\n",
      "[821] loss: 0.001 acc: 38.622\n",
      "[822] loss: 0.001 acc: 39.763\n",
      "[823] loss: 0.001 acc: 36.016\n",
      "[824] loss: 0.001 acc: 35.600\n",
      "[825] loss: 0.001 acc: 40.410\n",
      "[826] loss: 0.001 acc: 39.408\n",
      "[827] loss: 0.001 acc: 39.624\n",
      "[828] loss: 0.001 acc: 40.549\n",
      "[829] loss: 0.001 acc: 39.393\n",
      "[830] loss: 0.001 acc: 39.516\n",
      "[831] loss: 0.001 acc: 41.274\n",
      "[832] loss: 0.001 acc: 40.256\n",
      "[833] loss: 0.001 acc: 40.919\n",
      "[834] loss: 0.001 acc: 42.060\n",
      "[835] loss: 0.001 acc: 40.426\n",
      "[836] loss: 0.001 acc: 40.873\n",
      "[837] loss: 0.001 acc: 41.520\n",
      "[838] loss: 0.001 acc: 41.104\n",
      "[839] loss: 0.001 acc: 40.518\n",
      "[840] loss: 0.001 acc: 41.073\n",
      "[841] loss: 0.001 acc: 41.011\n",
      "[842] loss: 0.001 acc: 41.921\n",
      "[843] loss: 0.001 acc: 42.430\n",
      "[844] loss: 0.001 acc: 40.950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[845] loss: 0.001 acc: 40.888\n",
      "[846] loss: 0.001 acc: 42.029\n",
      "[847] loss: 0.001 acc: 42.661\n",
      "[848] loss: 0.001 acc: 42.507\n",
      "[849] loss: 0.001 acc: 42.245\n",
      "[850] loss: 0.001 acc: 42.276\n",
      "[851] loss: 0.001 acc: 42.199\n",
      "[852] loss: 0.001 acc: 41.998\n",
      "[853] loss: 0.001 acc: 42.414\n",
      "[854] loss: 0.001 acc: 42.291\n",
      "[855] loss: 0.001 acc: 42.322\n",
      "[856] loss: 0.001 acc: 42.584\n",
      "[857] loss: 0.001 acc: 42.754\n",
      "[858] loss: 0.001 acc: 42.245\n",
      "[859] loss: 0.001 acc: 42.476\n",
      "[860] loss: 0.001 acc: 42.692\n",
      "[861] loss: 0.001 acc: 42.738\n",
      "[862] loss: 0.001 acc: 42.985\n",
      "[863] loss: 0.001 acc: 42.553\n",
      "[864] loss: 0.001 acc: 42.569\n",
      "[865] loss: 0.001 acc: 43.139\n",
      "[866] loss: 0.001 acc: 42.877\n",
      "[867] loss: 0.001 acc: 42.815\n",
      "[868] loss: 0.001 acc: 42.969\n",
      "[869] loss: 0.001 acc: 42.862\n",
      "[870] loss: 0.001 acc: 43.278\n",
      "[871] loss: 0.001 acc: 43.232\n",
      "[872] loss: 0.001 acc: 43.108\n",
      "[873] loss: 0.001 acc: 43.386\n",
      "[874] loss: 0.001 acc: 43.525\n",
      "[875] loss: 0.001 acc: 43.478\n",
      "[876] loss: 0.001 acc: 43.586\n",
      "[877] loss: 0.001 acc: 43.725\n",
      "[878] loss: 0.001 acc: 43.833\n",
      "[879] loss: 0.001 acc: 43.879\n",
      "[880] loss: 0.001 acc: 44.080\n",
      "[881] loss: 0.001 acc: 44.203\n",
      "[882] loss: 0.001 acc: 44.218\n",
      "[883] loss: 0.001 acc: 44.665\n",
      "[884] loss: 0.001 acc: 44.496\n",
      "[885] loss: 0.001 acc: 44.820\n",
      "[886] loss: 0.001 acc: 45.051\n",
      "[887] loss: 0.001 acc: 45.298\n",
      "[888] loss: 0.001 acc: 45.220\n",
      "[889] loss: 0.001 acc: 45.359\n",
      "[890] loss: 0.001 acc: 45.344\n",
      "[891] loss: 0.001 acc: 45.483\n",
      "[892] loss: 0.001 acc: 45.560\n",
      "[893] loss: 0.001 acc: 45.745\n",
      "[894] loss: 0.001 acc: 45.914\n",
      "[895] loss: 0.001 acc: 45.853\n",
      "[896] loss: 0.001 acc: 45.853\n",
      "[897] loss: 0.001 acc: 46.176\n",
      "[898] loss: 0.001 acc: 46.130\n",
      "[899] loss: 0.001 acc: 46.392\n",
      "[900] loss: 0.001 acc: 46.654\n",
      "[901] loss: 0.001 acc: 46.500\n",
      "[902] loss: 0.001 acc: 46.716\n",
      "[903] loss: 0.001 acc: 46.716\n",
      "[904] loss: 0.001 acc: 46.577\n",
      "[905] loss: 0.001 acc: 46.809\n",
      "[906] loss: 0.001 acc: 46.423\n",
      "[907] loss: 0.001 acc: 46.809\n",
      "[908] loss: 0.001 acc: 46.361\n",
      "[909] loss: 0.001 acc: 46.824\n",
      "[910] loss: 0.001 acc: 46.253\n",
      "[911] loss: 0.001 acc: 46.685\n",
      "[912] loss: 0.001 acc: 46.192\n",
      "[913] loss: 0.001 acc: 46.747\n",
      "[914] loss: 0.001 acc: 45.868\n",
      "[915] loss: 0.001 acc: 46.192\n",
      "[916] loss: 0.001 acc: 45.082\n",
      "[917] loss: 0.001 acc: 45.544\n",
      "[918] loss: 0.001 acc: 44.403\n",
      "[919] loss: 0.001 acc: 44.434\n",
      "[920] loss: 0.001 acc: 44.172\n",
      "[921] loss: 0.001 acc: 43.417\n",
      "[922] loss: 0.001 acc: 43.540\n",
      "[923] loss: 0.001 acc: 43.170\n",
      "[924] loss: 0.001 acc: 44.141\n",
      "[925] loss: 0.001 acc: 41.721\n",
      "[926] loss: 0.001 acc: 43.401\n",
      "[927] loss: 0.001 acc: 41.813\n",
      "[928] loss: 0.001 acc: 33.935\n",
      "[929] loss: 0.001 acc: 43.771\n",
      "[930] loss: 0.001 acc: 40.086\n",
      "[931] loss: 0.001 acc: 34.551\n",
      "[932] loss: 0.001 acc: 40.487\n",
      "[933] loss: 0.001 acc: 39.809\n",
      "[934] loss: 0.001 acc: 42.923\n",
      "[935] loss: 0.001 acc: 34.613\n",
      "[936] loss: 0.001 acc: 42.414\n",
      "[937] loss: 0.001 acc: 40.241\n",
      "[938] loss: 0.001 acc: 42.507\n",
      "[939] loss: 0.001 acc: 40.102\n",
      "[940] loss: 0.001 acc: 37.928\n",
      "[941] loss: 0.001 acc: 44.835\n",
      "[942] loss: 0.001 acc: 42.738\n",
      "[943] loss: 0.001 acc: 43.833\n",
      "[944] loss: 0.001 acc: 40.903\n",
      "[945] loss: 0.001 acc: 41.119\n",
      "[946] loss: 0.001 acc: 42.723\n",
      "[947] loss: 0.001 acc: 41.690\n",
      "[948] loss: 0.001 acc: 42.661\n",
      "[949] loss: 0.001 acc: 44.388\n",
      "[950] loss: 0.001 acc: 44.820\n",
      "[951] loss: 0.001 acc: 44.249\n",
      "[952] loss: 0.001 acc: 44.958\n",
      "[953] loss: 0.001 acc: 44.419\n",
      "[954] loss: 0.001 acc: 44.588\n",
      "[955] loss: 0.001 acc: 45.066\n",
      "[956] loss: 0.001 acc: 45.513\n",
      "[957] loss: 0.001 acc: 44.789\n",
      "[958] loss: 0.001 acc: 44.527\n",
      "[959] loss: 0.001 acc: 44.157\n",
      "[960] loss: 0.001 acc: 44.480\n",
      "[961] loss: 0.001 acc: 45.282\n",
      "[962] loss: 0.001 acc: 45.513\n",
      "[963] loss: 0.001 acc: 45.914\n",
      "[964] loss: 0.001 acc: 45.344\n",
      "[965] loss: 0.001 acc: 44.619\n",
      "[966] loss: 0.001 acc: 45.390\n",
      "[967] loss: 0.001 acc: 46.207\n",
      "[968] loss: 0.001 acc: 45.714\n",
      "[969] loss: 0.001 acc: 46.115\n",
      "[970] loss: 0.001 acc: 45.729\n",
      "[971] loss: 0.001 acc: 45.313\n",
      "[972] loss: 0.001 acc: 45.344\n",
      "[973] loss: 0.001 acc: 46.269\n",
      "[974] loss: 0.001 acc: 46.546\n",
      "[975] loss: 0.001 acc: 46.516\n",
      "[976] loss: 0.001 acc: 46.331\n",
      "[977] loss: 0.001 acc: 45.991\n",
      "[978] loss: 0.001 acc: 46.331\n",
      "[979] loss: 0.001 acc: 46.793\n",
      "[980] loss: 0.001 acc: 47.194\n",
      "[981] loss: 0.001 acc: 46.916\n",
      "[982] loss: 0.001 acc: 46.747\n",
      "[983] loss: 0.001 acc: 46.408\n",
      "[984] loss: 0.001 acc: 47.024\n",
      "[985] loss: 0.001 acc: 47.348\n",
      "[986] loss: 0.001 acc: 47.055\n",
      "[987] loss: 0.001 acc: 47.148\n",
      "[988] loss: 0.001 acc: 46.978\n",
      "[989] loss: 0.001 acc: 47.240\n",
      "[990] loss: 0.001 acc: 47.117\n",
      "[991] loss: 0.001 acc: 47.487\n",
      "[992] loss: 0.001 acc: 47.256\n",
      "[993] loss: 0.001 acc: 47.410\n",
      "[994] loss: 0.001 acc: 47.132\n",
      "[995] loss: 0.001 acc: 47.471\n",
      "[996] loss: 0.001 acc: 47.148\n",
      "[997] loss: 0.001 acc: 47.286\n",
      "[998] loss: 0.001 acc: 47.549\n",
      "[999] loss: 0.001 acc: 47.502\n",
      "[1000] loss: 0.001 acc: 47.132\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 43 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(X_traintorch.float())\n",
    "    loss = criterion(outputs, y_traintorch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_traintorch.size(0)\n",
    "    correct += (predicted == y_traintorch).sum().item()\n",
    "    \n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f acc: %.3f' %\n",
    "              (epoch + 1, running_loss / 2000, (100 * correct / total)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_valtorch.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_valtorch.size(0)\n",
    "    correct += (predicted == y_valtorch).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:11:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.49827856025039124\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5, silence=False)\n",
    "XGB.fit(X_train, y_train)\n",
    "y_pred = XGB.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, np.around(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 3., ..., 1., 1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37996870109546166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "reg=LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred=reg.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val,y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442566510172144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DTree=DecisionTreeRegressor()\n",
    "DTree.fit(X_train,y_train)\n",
    "y_pred=DTree.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val,y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1436\n",
      "[LightGBM] [Info] Number of data points in the train set: 6486, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 2.491520\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "param = {'num_leaves':150, 'max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "\n",
    "val_data=lgb.Dataset(X_val,label=y_val)\n",
    "clf = lgb.train(param,train_data,100)\n",
    "y_pred=clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49295774647887325\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val,np.around(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06250000000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.12*0.4*0.4)/(.64*.6*.6 + .08*.6*.4 + .16*.6*.4 + .12*.4*.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.64*.6*.6 + 0.16*.6*.4)*.75/.2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12500000000000003"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.08*.6*.4 + 0.12*.4*.4)*.0625/(.08*.4*.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8124999999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.64*.6*.6 + 0.08*.6*.4)*.75/.2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18750000000000003"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.16*.6*.4 + 0.12*.4*.4)*.125/(.16*.4*.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0384/(.64*.6*.6 + .08*.6*.4 + .16*.6*.4 + .12*.4*.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "9676    2\n",
       "9677    2\n",
       "9678    0\n",
       "9679    1\n",
       "9680    1\n",
       "Name: cancellation_policy, Length: 9681, dtype: int32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[:,19] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.loc[:,features_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cancellation_policy'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_mean.pop(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>room_type</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>...</th>\n",
       "      <th>beds</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>extra_people</th>\n",
       "      <th>maximum_nights</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>require_guest_profile_picture</th>\n",
       "      <th>require_guest_phone_verification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>5</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4</td>\n",
       "      <td>346</td>\n",
       "      <td>2622</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2092</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2425</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>19</td>\n",
       "      <td>360</td>\n",
       "      <td>1082</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1052</td>\n",
       "      <td>3</td>\n",
       "      <td>351</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1</td>\n",
       "      <td>267</td>\n",
       "      <td>612</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>1.54</td>\n",
       "      <td>5</td>\n",
       "      <td>365</td>\n",
       "      <td>2081</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2092</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "      <td>328</td>\n",
       "      <td>467</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3403</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>28</td>\n",
       "      <td>174</td>\n",
       "      <td>2324</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1615</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>1659</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1395</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9681 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      neighbourhood  room_type  minimum_nights  number_of_reviews  \\\n",
       "0                21          0               1                170   \n",
       "1                 9          2               1                 11   \n",
       "2                27          0               3                  2   \n",
       "3                27          0               2                  1   \n",
       "4                31          0               2                 31   \n",
       "...             ...        ...             ...                ...   \n",
       "9676             26          0               7                  7   \n",
       "9677             17          0               1                  5   \n",
       "9678             21          0               3                  1   \n",
       "9679             27          0               2                 28   \n",
       "9680              9          2               2                  1   \n",
       "\n",
       "      last_review  reviews_per_month  calculated_host_listings_count  \\\n",
       "0               5               2.48                               4   \n",
       "1               7               0.57                               1   \n",
       "2              11               0.08                              19   \n",
       "3              11               0.13                               1   \n",
       "4              12               1.54                               5   \n",
       "...           ...                ...                             ...   \n",
       "9676            6               0.89                               2   \n",
       "9677            1               0.43                               1   \n",
       "9678            3               0.30                              28   \n",
       "9679            4               0.58                               1   \n",
       "9680            7               0.04                               1   \n",
       "\n",
       "      availability_365  host_since  host_is_superhost  ...  beds  bed_type  \\\n",
       "0                  346        2622                  1  ...     1         4   \n",
       "1                    0        2425                  1  ...     1         4   \n",
       "2                  360        1082                  0  ...     1         4   \n",
       "3                  267         612                  0  ...     3         4   \n",
       "4                  365        2081                  1  ...     3         4   \n",
       "...                ...         ...                ...  ...   ...       ...   \n",
       "9676               328         467                  0  ...     1         4   \n",
       "9677                 0        3403                  1  ...     5         4   \n",
       "9678               174        2324                  1  ...     2         4   \n",
       "9679                 0        1615                  0  ...     3         4   \n",
       "9680                88        1659                  0  ...     1         4   \n",
       "\n",
       "      cleaning_fee  guests_included  extra_people  maximum_nights  \\\n",
       "0             2092                1             0             365   \n",
       "1              558                1             0              15   \n",
       "2             1052                3           351              90   \n",
       "3                0                4             0            1125   \n",
       "4             2092                1             0             365   \n",
       "...            ...              ...           ...             ...   \n",
       "9676           700                1             0             185   \n",
       "9677          1500                6           500              30   \n",
       "9678          3492                1             0             180   \n",
       "9679           900                1             0            1125   \n",
       "9680          1395                1             0            1125   \n",
       "\n",
       "      instant_bookable  cancellation_policy  require_guest_profile_picture  \\\n",
       "0                    1                    2                              0   \n",
       "1                    0                    1                              0   \n",
       "2                    1                    1                              0   \n",
       "3                    1                    0                              0   \n",
       "4                    1                    1                              0   \n",
       "...                ...                  ...                            ...   \n",
       "9676                 0                    2                              0   \n",
       "9677                 1                    2                              0   \n",
       "9678                 1                    0                              0   \n",
       "9679                 1                    1                              0   \n",
       "9680                 0                    1                              0   \n",
       "\n",
       "      require_guest_phone_verification  \n",
       "0                                    0  \n",
       "1                                    0  \n",
       "2                                    0  \n",
       "3                                    0  \n",
       "4                                    0  \n",
       "...                                ...  \n",
       "9676                                 0  \n",
       "9677                                 0  \n",
       "9678                                 0  \n",
       "9679                                 0  \n",
       "9680                                 0  \n",
       "\n",
       "[9681 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
