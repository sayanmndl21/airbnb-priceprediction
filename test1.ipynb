{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, auc,roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import ttest_ind\n",
    "import time\n",
    "from datetime import date\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn import svm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['last_review', 'host_since'])\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = list(df.columns[1:24])\n",
    "df.columns.values.tolist()\n",
    "feat = [0,4,7,8,18,10,5,9,11,12,15]\n",
    "sort= [5,9]\n",
    "nah = [10,11,12,15]\n",
    "feature_list = []\n",
    "for i in feat:\n",
    "    feature_list += [features_mean[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is_business_travel_ready'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encodeval(dataframe,features_mean,col):\n",
    "    dataF = dataframe.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(dataF[features_mean[col]])\n",
    "    dataF[features_mean[col]] = le.transform(dataF[features_mean[col]])\n",
    "    return dataF\n",
    "\n",
    "df1['last_review'] = df['last_review'].dt.month\n",
    "df['host_since'] = pd.to_datetime(df['host_since']).dt.date\n",
    "df1['host_since'] = -(df['host_since']- date(2020, 10, 5)).dt.days\n",
    "df1 = encodeval(df1,features_mean,0)\n",
    "df1 = encodeval(df1,features_mean,1)\n",
    "df1 = encodeval(df1,features_mean,9)\n",
    "df1 = encodeval(df1,features_mean,13)\n",
    "df1 = encodeval(df1,features_mean,18)\n",
    "df1 = encodeval(df1,features_mean,19)\n",
    "df1 = encodeval(df1,features_mean,20)\n",
    "df1 = encodeval(df1,features_mean,21)\n",
    "df1 = encodeval(df1,features_mean,22)\n",
    "features_mean.pop(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.loc[:,features_mean]\n",
    "X = (X.iloc[:,0:] - X.iloc[:,0:].mean()) / X.iloc[:,0:].std() #standardise\n",
    "y = df1.loc[:, 'price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:07:05] 6486x22 matrix with 142692 entries loaded from dtrain.svm\n",
      "[18:07:05] 3195x22 matrix with 70290 entries loaded from dtest.svm\n"
     ]
    }
   ],
   "source": [
    "dump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\n",
    "dump_svmlight_file(X_val, y_val, 'dtest.svm', zero_based=True)\n",
    "dtrain_svm = xgb.DMatrix('dtrain.svm')\n",
    "dtest_svm = xgb.DMatrix('dtest.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 5}  # the number of classes that exist in this datset\n",
    "num_round = 100  # the number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540455695326385\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_val, best_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5311424100156494\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, best_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49076682316118936"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# creating tensor from targets_df \n",
    "X_traintorch = torch.tensor(X_train.values)\n",
    "y_traintorch = torch.tensor(y_train.values)\n",
    "\n",
    "X_valtorch = torch.tensor(X_val.values)\n",
    "y_valtorch = torch.tensor(y_val.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_features, num_class):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(num_features, num_features//2)\n",
    "        self.layer_out = nn.Linear(num_features//2, num_class) \n",
    "        \n",
    "        \n",
    "        #self.lr = nn.LeakyReLU()\n",
    "        self.bn = nn.BatchNorm1d(num_features//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn(x)\n",
    "        #x = self.lr(x)        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet(22,5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.001 acc: 13.537\n",
      "[2] loss: 0.002 acc: 22.387\n",
      "[3] loss: 0.004 acc: 25.254\n",
      "[4] loss: 0.003 acc: 35.338\n",
      "[5] loss: 0.003 acc: 38.915\n",
      "[6] loss: 0.003 acc: 34.413\n",
      "[7] loss: 0.003 acc: 25.979\n",
      "[8] loss: 0.002 acc: 37.142\n",
      "[9] loss: 0.002 acc: 33.935\n",
      "[10] loss: 0.001 acc: 29.695\n",
      "[11] loss: 0.001 acc: 37.882\n",
      "[12] loss: 0.001 acc: 39.578\n",
      "[13] loss: 0.001 acc: 31.221\n",
      "[14] loss: 0.001 acc: 29.988\n",
      "[15] loss: 0.001 acc: 36.247\n",
      "[16] loss: 0.001 acc: 38.452\n",
      "[17] loss: 0.001 acc: 34.289\n",
      "[18] loss: 0.001 acc: 38.961\n",
      "[19] loss: 0.001 acc: 44.018\n",
      "[20] loss: 0.001 acc: 40.965\n",
      "[21] loss: 0.001 acc: 35.353\n",
      "[22] loss: 0.001 acc: 42.291\n",
      "[23] loss: 0.001 acc: 41.906\n",
      "[24] loss: 0.001 acc: 40.056\n",
      "[25] loss: 0.001 acc: 46.469\n",
      "[26] loss: 0.001 acc: 44.619\n",
      "[27] loss: 0.001 acc: 44.172\n",
      "[28] loss: 0.001 acc: 46.747\n",
      "[29] loss: 0.001 acc: 42.538\n",
      "[30] loss: 0.001 acc: 42.985\n",
      "[31] loss: 0.001 acc: 42.754\n",
      "[32] loss: 0.001 acc: 44.989\n",
      "[33] loss: 0.001 acc: 41.751\n",
      "[34] loss: 0.001 acc: 44.527\n",
      "[35] loss: 0.001 acc: 44.928\n",
      "[36] loss: 0.001 acc: 43.247\n",
      "[37] loss: 0.001 acc: 42.492\n",
      "[38] loss: 0.001 acc: 45.097\n",
      "[39] loss: 0.001 acc: 45.698\n",
      "[40] loss: 0.001 acc: 45.822\n",
      "[41] loss: 0.001 acc: 46.824\n",
      "[42] loss: 0.001 acc: 46.809\n",
      "[43] loss: 0.001 acc: 46.747\n",
      "[44] loss: 0.001 acc: 46.639\n",
      "[45] loss: 0.001 acc: 47.132\n",
      "[46] loss: 0.001 acc: 46.546\n",
      "[47] loss: 0.001 acc: 46.146\n",
      "[48] loss: 0.001 acc: 47.240\n",
      "[49] loss: 0.001 acc: 48.242\n",
      "[50] loss: 0.001 acc: 47.502\n",
      "[51] loss: 0.001 acc: 48.273\n",
      "[52] loss: 0.001 acc: 48.227\n",
      "[53] loss: 0.001 acc: 47.379\n",
      "[54] loss: 0.001 acc: 47.718\n",
      "[55] loss: 0.001 acc: 48.397\n",
      "[56] loss: 0.001 acc: 47.888\n",
      "[57] loss: 0.001 acc: 48.181\n",
      "[58] loss: 0.001 acc: 48.350\n",
      "[59] loss: 0.001 acc: 48.227\n",
      "[60] loss: 0.001 acc: 47.903\n",
      "[61] loss: 0.001 acc: 48.319\n",
      "[62] loss: 0.001 acc: 48.535\n",
      "[63] loss: 0.001 acc: 48.319\n",
      "[64] loss: 0.001 acc: 48.227\n",
      "[65] loss: 0.001 acc: 48.921\n",
      "[66] loss: 0.001 acc: 48.782\n",
      "[67] loss: 0.001 acc: 48.612\n",
      "[68] loss: 0.001 acc: 48.874\n",
      "[69] loss: 0.001 acc: 48.659\n",
      "[70] loss: 0.001 acc: 48.597\n",
      "[71] loss: 0.001 acc: 49.198\n",
      "[72] loss: 0.001 acc: 48.952\n",
      "[73] loss: 0.001 acc: 48.874\n",
      "[74] loss: 0.001 acc: 48.921\n",
      "[75] loss: 0.001 acc: 49.229\n",
      "[76] loss: 0.001 acc: 49.645\n",
      "[77] loss: 0.001 acc: 49.013\n",
      "[78] loss: 0.001 acc: 48.674\n",
      "[79] loss: 0.001 acc: 49.337\n",
      "[80] loss: 0.001 acc: 48.828\n",
      "[81] loss: 0.001 acc: 49.090\n",
      "[82] loss: 0.001 acc: 49.106\n",
      "[83] loss: 0.001 acc: 48.905\n",
      "[84] loss: 0.001 acc: 48.936\n",
      "[85] loss: 0.001 acc: 48.982\n",
      "[86] loss: 0.001 acc: 49.152\n",
      "[87] loss: 0.001 acc: 49.229\n",
      "[88] loss: 0.001 acc: 48.936\n",
      "[89] loss: 0.001 acc: 49.013\n",
      "[90] loss: 0.001 acc: 48.982\n",
      "[91] loss: 0.001 acc: 49.044\n",
      "[92] loss: 0.001 acc: 49.075\n",
      "[93] loss: 0.001 acc: 49.060\n",
      "[94] loss: 0.001 acc: 49.152\n",
      "[95] loss: 0.001 acc: 48.967\n",
      "[96] loss: 0.001 acc: 49.029\n",
      "[97] loss: 0.001 acc: 49.090\n",
      "[98] loss: 0.001 acc: 49.183\n",
      "[99] loss: 0.001 acc: 49.075\n",
      "[100] loss: 0.001 acc: 49.198\n",
      "[101] loss: 0.001 acc: 49.106\n",
      "[102] loss: 0.001 acc: 49.245\n",
      "[103] loss: 0.001 acc: 49.352\n",
      "[104] loss: 0.001 acc: 49.106\n",
      "[105] loss: 0.001 acc: 49.322\n",
      "[106] loss: 0.001 acc: 49.214\n",
      "[107] loss: 0.001 acc: 49.198\n",
      "[108] loss: 0.001 acc: 49.029\n",
      "[109] loss: 0.001 acc: 49.260\n",
      "[110] loss: 0.001 acc: 48.797\n",
      "[111] loss: 0.001 acc: 49.214\n",
      "[112] loss: 0.001 acc: 48.936\n",
      "[113] loss: 0.001 acc: 48.813\n",
      "[114] loss: 0.001 acc: 48.181\n",
      "[115] loss: 0.001 acc: 47.487\n",
      "[116] loss: 0.001 acc: 46.608\n",
      "[117] loss: 0.001 acc: 46.855\n",
      "[118] loss: 0.001 acc: 45.606\n",
      "[119] loss: 0.001 acc: 47.055\n",
      "[120] loss: 0.001 acc: 47.934\n",
      "[121] loss: 0.001 acc: 48.998\n",
      "[122] loss: 0.001 acc: 47.919\n",
      "[123] loss: 0.001 acc: 46.916\n",
      "[124] loss: 0.001 acc: 47.179\n",
      "[125] loss: 0.001 acc: 48.319\n",
      "[126] loss: 0.001 acc: 48.828\n",
      "[127] loss: 0.001 acc: 46.916\n",
      "[128] loss: 0.001 acc: 46.778\n",
      "[129] loss: 0.001 acc: 48.905\n",
      "[130] loss: 0.001 acc: 49.152\n",
      "[131] loss: 0.001 acc: 47.286\n",
      "[132] loss: 0.001 acc: 47.163\n",
      "[133] loss: 0.001 acc: 49.198\n",
      "[134] loss: 0.001 acc: 48.134\n",
      "[135] loss: 0.001 acc: 47.687\n",
      "[136] loss: 0.001 acc: 49.229\n",
      "[137] loss: 0.001 acc: 47.965\n",
      "[138] loss: 0.001 acc: 47.795\n",
      "[139] loss: 0.001 acc: 48.936\n",
      "[140] loss: 0.001 acc: 48.057\n",
      "[141] loss: 0.001 acc: 48.289\n",
      "[142] loss: 0.001 acc: 49.460\n",
      "[143] loss: 0.001 acc: 47.518\n",
      "[144] loss: 0.001 acc: 48.582\n",
      "[145] loss: 0.001 acc: 49.645\n",
      "[146] loss: 0.001 acc: 47.394\n",
      "[147] loss: 0.001 acc: 48.998\n",
      "[148] loss: 0.001 acc: 49.060\n",
      "[149] loss: 0.001 acc: 48.366\n",
      "[150] loss: 0.001 acc: 49.275\n",
      "[151] loss: 0.001 acc: 49.013\n",
      "[152] loss: 0.001 acc: 48.952\n",
      "[153] loss: 0.001 acc: 48.921\n",
      "[154] loss: 0.001 acc: 49.383\n",
      "[155] loss: 0.001 acc: 49.306\n",
      "[156] loss: 0.001 acc: 48.874\n",
      "[157] loss: 0.001 acc: 49.661\n",
      "[158] loss: 0.001 acc: 49.537\n",
      "[159] loss: 0.001 acc: 49.198\n",
      "[160] loss: 0.001 acc: 49.414\n",
      "[161] loss: 0.001 acc: 49.692\n",
      "[162] loss: 0.001 acc: 49.029\n",
      "[163] loss: 0.001 acc: 49.090\n",
      "[164] loss: 0.001 acc: 49.738\n",
      "[165] loss: 0.001 acc: 49.414\n",
      "[166] loss: 0.001 acc: 49.137\n",
      "[167] loss: 0.001 acc: 49.399\n",
      "[168] loss: 0.001 acc: 49.522\n",
      "[169] loss: 0.001 acc: 49.029\n",
      "[170] loss: 0.001 acc: 49.229\n",
      "[171] loss: 0.001 acc: 49.507\n",
      "[172] loss: 0.001 acc: 49.152\n",
      "[173] loss: 0.001 acc: 49.060\n",
      "[174] loss: 0.001 acc: 49.645\n",
      "[175] loss: 0.001 acc: 49.645\n",
      "[176] loss: 0.001 acc: 49.152\n",
      "[177] loss: 0.001 acc: 49.291\n",
      "[178] loss: 0.001 acc: 49.645\n",
      "[179] loss: 0.001 acc: 49.214\n",
      "[180] loss: 0.001 acc: 49.137\n",
      "[181] loss: 0.001 acc: 49.692\n",
      "[182] loss: 0.001 acc: 49.476\n",
      "[183] loss: 0.001 acc: 49.306\n",
      "[184] loss: 0.001 acc: 49.645\n",
      "[185] loss: 0.001 acc: 49.707\n",
      "[186] loss: 0.001 acc: 49.245\n",
      "[187] loss: 0.001 acc: 49.306\n",
      "[188] loss: 0.001 acc: 49.707\n",
      "[189] loss: 0.001 acc: 49.306\n",
      "[190] loss: 0.001 acc: 49.337\n",
      "[191] loss: 0.001 acc: 49.769\n",
      "[192] loss: 0.001 acc: 49.337\n",
      "[193] loss: 0.001 acc: 49.260\n",
      "[194] loss: 0.001 acc: 49.692\n",
      "[195] loss: 0.001 acc: 49.599\n",
      "[196] loss: 0.001 acc: 49.352\n",
      "[197] loss: 0.001 acc: 49.491\n",
      "[198] loss: 0.001 acc: 49.645\n",
      "[199] loss: 0.001 acc: 49.368\n",
      "[200] loss: 0.001 acc: 49.553\n",
      "[201] loss: 0.001 acc: 49.692\n",
      "[202] loss: 0.001 acc: 49.414\n",
      "[203] loss: 0.001 acc: 49.491\n",
      "[204] loss: 0.001 acc: 49.676\n",
      "[205] loss: 0.001 acc: 49.568\n",
      "[206] loss: 0.001 acc: 49.460\n",
      "[207] loss: 0.001 acc: 49.676\n",
      "[208] loss: 0.001 acc: 49.615\n",
      "[209] loss: 0.001 acc: 49.491\n",
      "[210] loss: 0.001 acc: 49.800\n",
      "[211] loss: 0.001 acc: 49.630\n",
      "[212] loss: 0.001 acc: 49.460\n",
      "[213] loss: 0.001 acc: 49.676\n",
      "[214] loss: 0.001 acc: 49.676\n",
      "[215] loss: 0.001 acc: 49.414\n",
      "[216] loss: 0.001 acc: 49.645\n",
      "[217] loss: 0.001 acc: 49.645\n",
      "[218] loss: 0.001 acc: 49.491\n",
      "[219] loss: 0.001 acc: 49.537\n",
      "[220] loss: 0.001 acc: 49.599\n",
      "[221] loss: 0.001 acc: 49.476\n",
      "[222] loss: 0.001 acc: 49.460\n",
      "[223] loss: 0.001 acc: 49.615\n",
      "[224] loss: 0.001 acc: 49.445\n",
      "[225] loss: 0.001 acc: 49.460\n",
      "[226] loss: 0.001 acc: 49.522\n",
      "[227] loss: 0.001 acc: 49.430\n",
      "[228] loss: 0.001 acc: 49.491\n",
      "[229] loss: 0.001 acc: 49.522\n",
      "[230] loss: 0.001 acc: 49.445\n",
      "[231] loss: 0.001 acc: 49.430\n",
      "[232] loss: 0.001 acc: 49.568\n",
      "[233] loss: 0.001 acc: 49.445\n",
      "[234] loss: 0.001 acc: 49.445\n",
      "[235] loss: 0.001 acc: 49.445\n",
      "[236] loss: 0.001 acc: 49.430\n",
      "[237] loss: 0.001 acc: 49.430\n",
      "[238] loss: 0.001 acc: 49.476\n",
      "[239] loss: 0.001 acc: 49.460\n",
      "[240] loss: 0.001 acc: 49.445\n",
      "[241] loss: 0.001 acc: 49.460\n",
      "[242] loss: 0.001 acc: 49.460\n",
      "[243] loss: 0.001 acc: 49.522\n",
      "[244] loss: 0.001 acc: 49.507\n",
      "[245] loss: 0.001 acc: 49.507\n",
      "[246] loss: 0.001 acc: 49.584\n",
      "[247] loss: 0.001 acc: 49.568\n",
      "[248] loss: 0.001 acc: 49.553\n",
      "[249] loss: 0.001 acc: 49.599\n",
      "[250] loss: 0.001 acc: 49.584\n",
      "[251] loss: 0.001 acc: 49.584\n",
      "[252] loss: 0.001 acc: 49.568\n",
      "[253] loss: 0.001 acc: 49.599\n",
      "[254] loss: 0.001 acc: 49.568\n",
      "[255] loss: 0.001 acc: 49.568\n",
      "[256] loss: 0.001 acc: 49.507\n",
      "[257] loss: 0.001 acc: 49.507\n",
      "[258] loss: 0.001 acc: 49.522\n",
      "[259] loss: 0.001 acc: 49.460\n",
      "[260] loss: 0.001 acc: 49.491\n",
      "[261] loss: 0.001 acc: 49.491\n",
      "[262] loss: 0.001 acc: 49.507\n",
      "[263] loss: 0.001 acc: 49.522\n",
      "[264] loss: 0.001 acc: 49.553\n",
      "[265] loss: 0.001 acc: 49.553\n",
      "[266] loss: 0.001 acc: 49.568\n",
      "[267] loss: 0.001 acc: 49.584\n",
      "[268] loss: 0.001 acc: 49.584\n",
      "[269] loss: 0.001 acc: 49.615\n",
      "[270] loss: 0.001 acc: 49.584\n",
      "[271] loss: 0.001 acc: 49.584\n",
      "[272] loss: 0.001 acc: 49.584\n",
      "[273] loss: 0.001 acc: 49.553\n",
      "[274] loss: 0.001 acc: 49.537\n",
      "[275] loss: 0.001 acc: 49.522\n",
      "[276] loss: 0.001 acc: 49.522\n",
      "[277] loss: 0.001 acc: 49.568\n",
      "[278] loss: 0.001 acc: 49.568\n",
      "[279] loss: 0.001 acc: 49.568\n",
      "[280] loss: 0.001 acc: 49.553\n",
      "[281] loss: 0.001 acc: 49.537\n",
      "[282] loss: 0.001 acc: 49.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283] loss: 0.001 acc: 49.522\n",
      "[284] loss: 0.001 acc: 49.537\n",
      "[285] loss: 0.001 acc: 49.553\n",
      "[286] loss: 0.001 acc: 49.537\n",
      "[287] loss: 0.001 acc: 49.507\n",
      "[288] loss: 0.001 acc: 49.507\n",
      "[289] loss: 0.001 acc: 49.507\n",
      "[290] loss: 0.001 acc: 49.522\n",
      "[291] loss: 0.001 acc: 49.507\n",
      "[292] loss: 0.001 acc: 49.522\n",
      "[293] loss: 0.001 acc: 49.537\n",
      "[294] loss: 0.001 acc: 49.537\n",
      "[295] loss: 0.001 acc: 49.522\n",
      "[296] loss: 0.001 acc: 49.522\n",
      "[297] loss: 0.001 acc: 49.553\n",
      "[298] loss: 0.001 acc: 49.537\n",
      "[299] loss: 0.001 acc: 49.522\n",
      "[300] loss: 0.001 acc: 49.522\n",
      "[301] loss: 0.001 acc: 49.507\n",
      "[302] loss: 0.001 acc: 49.491\n",
      "[303] loss: 0.001 acc: 49.507\n",
      "[304] loss: 0.001 acc: 49.507\n",
      "[305] loss: 0.001 acc: 49.507\n",
      "[306] loss: 0.001 acc: 49.491\n",
      "[307] loss: 0.001 acc: 49.491\n",
      "[308] loss: 0.001 acc: 49.491\n",
      "[309] loss: 0.001 acc: 49.460\n",
      "[310] loss: 0.001 acc: 49.460\n",
      "[311] loss: 0.001 acc: 49.476\n",
      "[312] loss: 0.001 acc: 49.476\n",
      "[313] loss: 0.001 acc: 49.460\n",
      "[314] loss: 0.001 acc: 49.460\n",
      "[315] loss: 0.001 acc: 49.476\n",
      "[316] loss: 0.001 acc: 49.491\n",
      "[317] loss: 0.001 acc: 49.507\n",
      "[318] loss: 0.001 acc: 49.537\n",
      "[319] loss: 0.001 acc: 49.522\n",
      "[320] loss: 0.001 acc: 49.537\n",
      "[321] loss: 0.001 acc: 49.522\n",
      "[322] loss: 0.001 acc: 49.537\n",
      "[323] loss: 0.001 acc: 49.553\n",
      "[324] loss: 0.001 acc: 49.537\n",
      "[325] loss: 0.001 acc: 49.553\n",
      "[326] loss: 0.001 acc: 49.537\n",
      "[327] loss: 0.001 acc: 49.553\n",
      "[328] loss: 0.001 acc: 49.553\n",
      "[329] loss: 0.001 acc: 49.568\n",
      "[330] loss: 0.001 acc: 49.553\n",
      "[331] loss: 0.001 acc: 49.568\n",
      "[332] loss: 0.001 acc: 49.553\n",
      "[333] loss: 0.001 acc: 49.553\n",
      "[334] loss: 0.001 acc: 49.553\n",
      "[335] loss: 0.001 acc: 49.568\n",
      "[336] loss: 0.001 acc: 49.584\n",
      "[337] loss: 0.001 acc: 49.568\n",
      "[338] loss: 0.001 acc: 49.584\n",
      "[339] loss: 0.001 acc: 49.599\n",
      "[340] loss: 0.001 acc: 49.584\n",
      "[341] loss: 0.001 acc: 49.599\n",
      "[342] loss: 0.001 acc: 49.615\n",
      "[343] loss: 0.001 acc: 49.615\n",
      "[344] loss: 0.001 acc: 49.599\n",
      "[345] loss: 0.001 acc: 49.584\n",
      "[346] loss: 0.001 acc: 49.584\n",
      "[347] loss: 0.001 acc: 49.584\n",
      "[348] loss: 0.001 acc: 49.553\n",
      "[349] loss: 0.001 acc: 49.553\n",
      "[350] loss: 0.001 acc: 49.553\n",
      "[351] loss: 0.001 acc: 49.553\n",
      "[352] loss: 0.001 acc: 49.553\n",
      "[353] loss: 0.001 acc: 49.553\n",
      "[354] loss: 0.001 acc: 49.553\n",
      "[355] loss: 0.001 acc: 49.537\n",
      "[356] loss: 0.001 acc: 49.568\n",
      "[357] loss: 0.001 acc: 49.568\n",
      "[358] loss: 0.001 acc: 49.584\n",
      "[359] loss: 0.001 acc: 49.584\n",
      "[360] loss: 0.001 acc: 49.584\n",
      "[361] loss: 0.001 acc: 49.568\n",
      "[362] loss: 0.001 acc: 49.568\n",
      "[363] loss: 0.001 acc: 49.568\n",
      "[364] loss: 0.001 acc: 49.599\n",
      "[365] loss: 0.001 acc: 49.599\n",
      "[366] loss: 0.001 acc: 49.599\n",
      "[367] loss: 0.001 acc: 49.584\n",
      "[368] loss: 0.001 acc: 49.584\n",
      "[369] loss: 0.001 acc: 49.584\n",
      "[370] loss: 0.001 acc: 49.584\n",
      "[371] loss: 0.001 acc: 49.599\n",
      "[372] loss: 0.001 acc: 49.615\n",
      "[373] loss: 0.001 acc: 49.599\n",
      "[374] loss: 0.001 acc: 49.584\n",
      "[375] loss: 0.001 acc: 49.584\n",
      "[376] loss: 0.001 acc: 49.568\n",
      "[377] loss: 0.001 acc: 49.584\n",
      "[378] loss: 0.001 acc: 49.568\n",
      "[379] loss: 0.001 acc: 49.584\n",
      "[380] loss: 0.001 acc: 49.584\n",
      "[381] loss: 0.001 acc: 49.599\n",
      "[382] loss: 0.001 acc: 49.584\n",
      "[383] loss: 0.001 acc: 49.599\n",
      "[384] loss: 0.001 acc: 49.553\n",
      "[385] loss: 0.001 acc: 49.615\n",
      "[386] loss: 0.001 acc: 49.553\n",
      "[387] loss: 0.001 acc: 49.584\n",
      "[388] loss: 0.001 acc: 49.553\n",
      "[389] loss: 0.001 acc: 49.568\n",
      "[390] loss: 0.001 acc: 49.537\n",
      "[391] loss: 0.001 acc: 49.568\n",
      "[392] loss: 0.001 acc: 49.553\n",
      "[393] loss: 0.001 acc: 49.507\n",
      "[394] loss: 0.001 acc: 49.522\n",
      "[395] loss: 0.001 acc: 49.491\n",
      "[396] loss: 0.001 acc: 49.784\n",
      "[397] loss: 0.001 acc: 49.537\n",
      "[398] loss: 0.001 acc: 49.861\n",
      "[399] loss: 0.001 acc: 49.584\n",
      "[400] loss: 0.001 acc: 49.892\n",
      "[401] loss: 0.001 acc: 49.383\n",
      "[402] loss: 0.001 acc: 49.830\n",
      "[403] loss: 0.001 acc: 48.042\n",
      "[404] loss: 0.001 acc: 48.027\n",
      "[405] loss: 0.001 acc: 46.716\n",
      "[406] loss: 0.001 acc: 46.701\n",
      "[407] loss: 0.001 acc: 45.683\n",
      "[408] loss: 0.001 acc: 47.148\n",
      "[409] loss: 0.001 acc: 48.705\n",
      "[410] loss: 0.001 acc: 48.535\n",
      "[411] loss: 0.001 acc: 47.949\n",
      "[412] loss: 0.001 acc: 48.181\n",
      "[413] loss: 0.001 acc: 49.275\n",
      "[414] loss: 0.001 acc: 49.337\n",
      "[415] loss: 0.001 acc: 48.566\n",
      "[416] loss: 0.001 acc: 49.630\n",
      "[417] loss: 0.001 acc: 49.707\n",
      "[418] loss: 0.001 acc: 48.859\n",
      "[419] loss: 0.001 acc: 49.707\n",
      "[420] loss: 0.001 acc: 49.985\n",
      "[421] loss: 0.001 acc: 49.152\n",
      "[422] loss: 0.001 acc: 49.214\n",
      "[423] loss: 0.001 acc: 50.231\n",
      "[424] loss: 0.001 acc: 49.445\n",
      "[425] loss: 0.001 acc: 49.013\n",
      "[426] loss: 0.001 acc: 49.861\n",
      "[427] loss: 0.001 acc: 49.753\n",
      "[428] loss: 0.001 acc: 49.245\n",
      "[429] loss: 0.001 acc: 49.676\n",
      "[430] loss: 0.001 acc: 49.985\n",
      "[431] loss: 0.001 acc: 49.553\n",
      "[432] loss: 0.001 acc: 49.460\n",
      "[433] loss: 0.001 acc: 49.769\n",
      "[434] loss: 0.001 acc: 49.445\n",
      "[435] loss: 0.001 acc: 49.537\n",
      "[436] loss: 0.001 acc: 49.661\n",
      "[437] loss: 0.001 acc: 49.815\n",
      "[438] loss: 0.001 acc: 49.460\n",
      "[439] loss: 0.001 acc: 49.445\n",
      "[440] loss: 0.001 acc: 49.877\n",
      "[441] loss: 0.001 acc: 49.537\n",
      "[442] loss: 0.001 acc: 49.430\n",
      "[443] loss: 0.001 acc: 49.476\n",
      "[444] loss: 0.001 acc: 49.985\n",
      "[445] loss: 0.001 acc: 49.692\n",
      "[446] loss: 0.001 acc: 49.522\n",
      "[447] loss: 0.001 acc: 49.707\n",
      "[448] loss: 0.001 acc: 49.784\n",
      "[449] loss: 0.001 acc: 49.676\n",
      "[450] loss: 0.001 acc: 49.507\n",
      "[451] loss: 0.001 acc: 49.769\n",
      "[452] loss: 0.001 acc: 49.584\n",
      "[453] loss: 0.001 acc: 49.676\n",
      "[454] loss: 0.001 acc: 49.722\n",
      "[455] loss: 0.001 acc: 49.707\n",
      "[456] loss: 0.001 acc: 49.645\n",
      "[457] loss: 0.001 acc: 49.584\n",
      "[458] loss: 0.001 acc: 49.722\n",
      "[459] loss: 0.001 acc: 49.568\n",
      "[460] loss: 0.001 acc: 49.753\n",
      "[461] loss: 0.001 acc: 49.630\n",
      "[462] loss: 0.001 acc: 49.615\n",
      "[463] loss: 0.001 acc: 49.460\n",
      "[464] loss: 0.001 acc: 49.568\n",
      "[465] loss: 0.001 acc: 49.584\n",
      "[466] loss: 0.001 acc: 49.661\n",
      "[467] loss: 0.001 acc: 49.460\n",
      "[468] loss: 0.001 acc: 49.522\n",
      "[469] loss: 0.001 acc: 49.645\n",
      "[470] loss: 0.001 acc: 49.568\n",
      "[471] loss: 0.001 acc: 49.537\n",
      "[472] loss: 0.001 acc: 49.568\n",
      "[473] loss: 0.001 acc: 49.553\n",
      "[474] loss: 0.001 acc: 49.599\n",
      "[475] loss: 0.001 acc: 49.507\n",
      "[476] loss: 0.001 acc: 49.584\n",
      "[477] loss: 0.001 acc: 49.522\n",
      "[478] loss: 0.001 acc: 49.537\n",
      "[479] loss: 0.001 acc: 49.568\n",
      "[480] loss: 0.001 acc: 49.553\n",
      "[481] loss: 0.001 acc: 49.460\n",
      "[482] loss: 0.001 acc: 49.568\n",
      "[483] loss: 0.001 acc: 49.568\n",
      "[484] loss: 0.001 acc: 49.537\n",
      "[485] loss: 0.001 acc: 49.568\n",
      "[486] loss: 0.001 acc: 49.522\n",
      "[487] loss: 0.001 acc: 49.537\n",
      "[488] loss: 0.001 acc: 49.537\n",
      "[489] loss: 0.001 acc: 49.553\n",
      "[490] loss: 0.001 acc: 49.507\n",
      "[491] loss: 0.001 acc: 49.537\n",
      "[492] loss: 0.001 acc: 49.599\n",
      "[493] loss: 0.001 acc: 49.537\n",
      "[494] loss: 0.001 acc: 49.460\n",
      "[495] loss: 0.001 acc: 49.568\n",
      "[496] loss: 0.001 acc: 49.599\n",
      "[497] loss: 0.001 acc: 49.476\n",
      "[498] loss: 0.001 acc: 49.676\n",
      "[499] loss: 0.001 acc: 49.615\n",
      "[500] loss: 0.001 acc: 49.507\n",
      "[501] loss: 0.001 acc: 49.553\n",
      "[502] loss: 0.001 acc: 49.615\n",
      "[503] loss: 0.001 acc: 49.553\n",
      "[504] loss: 0.001 acc: 49.522\n",
      "[505] loss: 0.001 acc: 49.645\n",
      "[506] loss: 0.001 acc: 49.537\n",
      "[507] loss: 0.001 acc: 49.476\n",
      "[508] loss: 0.001 acc: 49.599\n",
      "[509] loss: 0.001 acc: 49.568\n",
      "[510] loss: 0.001 acc: 49.522\n",
      "[511] loss: 0.001 acc: 49.599\n",
      "[512] loss: 0.001 acc: 49.615\n",
      "[513] loss: 0.001 acc: 49.522\n",
      "[514] loss: 0.001 acc: 49.568\n",
      "[515] loss: 0.001 acc: 49.599\n",
      "[516] loss: 0.001 acc: 49.537\n",
      "[517] loss: 0.001 acc: 49.599\n",
      "[518] loss: 0.001 acc: 49.553\n",
      "[519] loss: 0.001 acc: 49.507\n",
      "[520] loss: 0.001 acc: 49.630\n",
      "[521] loss: 0.001 acc: 49.568\n",
      "[522] loss: 0.001 acc: 49.553\n",
      "[523] loss: 0.001 acc: 49.599\n",
      "[524] loss: 0.001 acc: 49.615\n",
      "[525] loss: 0.001 acc: 49.568\n",
      "[526] loss: 0.001 acc: 49.584\n",
      "[527] loss: 0.001 acc: 49.615\n",
      "[528] loss: 0.001 acc: 49.568\n",
      "[529] loss: 0.001 acc: 49.615\n",
      "[530] loss: 0.001 acc: 49.615\n",
      "[531] loss: 0.001 acc: 49.584\n",
      "[532] loss: 0.001 acc: 49.584\n",
      "[533] loss: 0.001 acc: 49.630\n",
      "[534] loss: 0.001 acc: 49.584\n",
      "[535] loss: 0.001 acc: 49.584\n",
      "[536] loss: 0.001 acc: 49.630\n",
      "[537] loss: 0.001 acc: 49.584\n",
      "[538] loss: 0.001 acc: 49.584\n",
      "[539] loss: 0.001 acc: 49.615\n",
      "[540] loss: 0.001 acc: 49.584\n",
      "[541] loss: 0.001 acc: 49.584\n",
      "[542] loss: 0.001 acc: 49.599\n",
      "[543] loss: 0.001 acc: 49.584\n",
      "[544] loss: 0.001 acc: 49.584\n",
      "[545] loss: 0.001 acc: 49.615\n",
      "[546] loss: 0.001 acc: 49.584\n",
      "[547] loss: 0.001 acc: 49.584\n",
      "[548] loss: 0.001 acc: 49.599\n",
      "[549] loss: 0.001 acc: 49.584\n",
      "[550] loss: 0.001 acc: 49.584\n",
      "[551] loss: 0.001 acc: 49.599\n",
      "[552] loss: 0.001 acc: 49.599\n",
      "[553] loss: 0.001 acc: 49.584\n",
      "[554] loss: 0.001 acc: 49.615\n",
      "[555] loss: 0.001 acc: 49.599\n",
      "[556] loss: 0.001 acc: 49.599\n",
      "[557] loss: 0.001 acc: 49.615\n",
      "[558] loss: 0.001 acc: 49.599\n",
      "[559] loss: 0.001 acc: 49.599\n",
      "[560] loss: 0.001 acc: 49.615\n",
      "[561] loss: 0.001 acc: 49.599\n",
      "[562] loss: 0.001 acc: 49.599\n",
      "[563] loss: 0.001 acc: 49.599\n",
      "[564] loss: 0.001 acc: 49.599\n",
      "[565] loss: 0.001 acc: 49.599\n",
      "[566] loss: 0.001 acc: 49.599\n",
      "[567] loss: 0.001 acc: 49.599\n",
      "[568] loss: 0.001 acc: 49.599\n",
      "[569] loss: 0.001 acc: 49.584\n",
      "[570] loss: 0.001 acc: 49.584\n",
      "[571] loss: 0.001 acc: 49.599\n",
      "[572] loss: 0.001 acc: 49.584\n",
      "[573] loss: 0.001 acc: 49.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[574] loss: 0.001 acc: 49.584\n",
      "[575] loss: 0.001 acc: 49.584\n",
      "[576] loss: 0.001 acc: 49.584\n",
      "[577] loss: 0.001 acc: 49.584\n",
      "[578] loss: 0.001 acc: 49.584\n",
      "[579] loss: 0.001 acc: 49.584\n",
      "[580] loss: 0.001 acc: 49.584\n",
      "[581] loss: 0.001 acc: 49.584\n",
      "[582] loss: 0.001 acc: 49.584\n",
      "[583] loss: 0.001 acc: 49.584\n",
      "[584] loss: 0.001 acc: 49.584\n",
      "[585] loss: 0.001 acc: 49.584\n",
      "[586] loss: 0.001 acc: 49.584\n",
      "[587] loss: 0.001 acc: 49.584\n",
      "[588] loss: 0.001 acc: 49.584\n",
      "[589] loss: 0.001 acc: 49.584\n",
      "[590] loss: 0.001 acc: 49.584\n",
      "[591] loss: 0.001 acc: 49.584\n",
      "[592] loss: 0.001 acc: 49.584\n",
      "[593] loss: 0.001 acc: 49.584\n",
      "[594] loss: 0.001 acc: 49.584\n",
      "[595] loss: 0.001 acc: 49.584\n",
      "[596] loss: 0.001 acc: 49.584\n",
      "[597] loss: 0.001 acc: 49.584\n",
      "[598] loss: 0.001 acc: 49.584\n",
      "[599] loss: 0.001 acc: 49.584\n",
      "[600] loss: 0.001 acc: 49.584\n",
      "[601] loss: 0.001 acc: 49.584\n",
      "[602] loss: 0.001 acc: 49.584\n",
      "[603] loss: 0.001 acc: 49.584\n",
      "[604] loss: 0.001 acc: 49.584\n",
      "[605] loss: 0.001 acc: 49.584\n",
      "[606] loss: 0.001 acc: 49.584\n",
      "[607] loss: 0.001 acc: 49.584\n",
      "[608] loss: 0.001 acc: 49.584\n",
      "[609] loss: 0.001 acc: 49.584\n",
      "[610] loss: 0.001 acc: 49.584\n",
      "[611] loss: 0.001 acc: 49.584\n",
      "[612] loss: 0.001 acc: 49.584\n",
      "[613] loss: 0.001 acc: 49.584\n",
      "[614] loss: 0.001 acc: 49.584\n",
      "[615] loss: 0.001 acc: 49.584\n",
      "[616] loss: 0.001 acc: 49.584\n",
      "[617] loss: 0.001 acc: 49.584\n",
      "[618] loss: 0.001 acc: 49.584\n",
      "[619] loss: 0.001 acc: 49.584\n",
      "[620] loss: 0.001 acc: 49.584\n",
      "[621] loss: 0.001 acc: 49.584\n",
      "[622] loss: 0.001 acc: 49.584\n",
      "[623] loss: 0.001 acc: 49.584\n",
      "[624] loss: 0.001 acc: 49.584\n",
      "[625] loss: 0.001 acc: 49.584\n",
      "[626] loss: 0.001 acc: 49.584\n",
      "[627] loss: 0.001 acc: 49.584\n",
      "[628] loss: 0.001 acc: 49.584\n",
      "[629] loss: 0.001 acc: 49.584\n",
      "[630] loss: 0.001 acc: 49.584\n",
      "[631] loss: 0.001 acc: 49.584\n",
      "[632] loss: 0.001 acc: 49.584\n",
      "[633] loss: 0.001 acc: 49.584\n",
      "[634] loss: 0.001 acc: 49.584\n",
      "[635] loss: 0.001 acc: 49.584\n",
      "[636] loss: 0.001 acc: 49.584\n",
      "[637] loss: 0.001 acc: 49.584\n",
      "[638] loss: 0.001 acc: 49.584\n",
      "[639] loss: 0.001 acc: 49.584\n",
      "[640] loss: 0.001 acc: 49.584\n",
      "[641] loss: 0.001 acc: 49.584\n",
      "[642] loss: 0.001 acc: 49.584\n",
      "[643] loss: 0.001 acc: 49.584\n",
      "[644] loss: 0.001 acc: 49.584\n",
      "[645] loss: 0.001 acc: 49.584\n",
      "[646] loss: 0.001 acc: 49.584\n",
      "[647] loss: 0.001 acc: 49.584\n",
      "[648] loss: 0.001 acc: 49.584\n",
      "[649] loss: 0.001 acc: 49.584\n",
      "[650] loss: 0.001 acc: 49.584\n",
      "[651] loss: 0.001 acc: 49.584\n",
      "[652] loss: 0.001 acc: 49.584\n",
      "[653] loss: 0.001 acc: 49.584\n",
      "[654] loss: 0.001 acc: 49.599\n",
      "[655] loss: 0.001 acc: 49.584\n",
      "[656] loss: 0.001 acc: 49.599\n",
      "[657] loss: 0.001 acc: 49.599\n",
      "[658] loss: 0.001 acc: 49.599\n",
      "[659] loss: 0.001 acc: 49.599\n",
      "[660] loss: 0.001 acc: 49.599\n",
      "[661] loss: 0.001 acc: 49.599\n",
      "[662] loss: 0.001 acc: 49.599\n",
      "[663] loss: 0.001 acc: 49.599\n",
      "[664] loss: 0.001 acc: 49.599\n",
      "[665] loss: 0.001 acc: 49.599\n",
      "[666] loss: 0.001 acc: 49.599\n",
      "[667] loss: 0.001 acc: 49.599\n",
      "[668] loss: 0.001 acc: 49.599\n",
      "[669] loss: 0.001 acc: 49.599\n",
      "[670] loss: 0.001 acc: 49.599\n",
      "[671] loss: 0.001 acc: 49.599\n",
      "[672] loss: 0.001 acc: 49.599\n",
      "[673] loss: 0.001 acc: 49.599\n",
      "[674] loss: 0.001 acc: 49.599\n",
      "[675] loss: 0.001 acc: 49.599\n",
      "[676] loss: 0.001 acc: 49.599\n",
      "[677] loss: 0.001 acc: 49.599\n",
      "[678] loss: 0.001 acc: 49.599\n",
      "[679] loss: 0.001 acc: 49.599\n",
      "[680] loss: 0.001 acc: 49.599\n",
      "[681] loss: 0.001 acc: 49.599\n",
      "[682] loss: 0.001 acc: 49.599\n",
      "[683] loss: 0.001 acc: 49.599\n",
      "[684] loss: 0.001 acc: 49.599\n",
      "[685] loss: 0.001 acc: 49.599\n",
      "[686] loss: 0.001 acc: 49.599\n",
      "[687] loss: 0.001 acc: 49.599\n",
      "[688] loss: 0.001 acc: 49.599\n",
      "[689] loss: 0.001 acc: 49.599\n",
      "[690] loss: 0.001 acc: 49.599\n",
      "[691] loss: 0.001 acc: 49.599\n",
      "[692] loss: 0.001 acc: 49.599\n",
      "[693] loss: 0.001 acc: 49.599\n",
      "[694] loss: 0.001 acc: 49.599\n",
      "[695] loss: 0.001 acc: 49.599\n",
      "[696] loss: 0.001 acc: 49.599\n",
      "[697] loss: 0.001 acc: 49.599\n",
      "[698] loss: 0.001 acc: 49.599\n",
      "[699] loss: 0.001 acc: 49.599\n",
      "[700] loss: 0.001 acc: 49.599\n",
      "[701] loss: 0.001 acc: 49.599\n",
      "[702] loss: 0.001 acc: 49.599\n",
      "[703] loss: 0.001 acc: 49.599\n",
      "[704] loss: 0.001 acc: 49.599\n",
      "[705] loss: 0.001 acc: 49.599\n",
      "[706] loss: 0.001 acc: 49.599\n",
      "[707] loss: 0.001 acc: 49.599\n",
      "[708] loss: 0.001 acc: 49.599\n",
      "[709] loss: 0.001 acc: 49.599\n",
      "[710] loss: 0.001 acc: 49.599\n",
      "[711] loss: 0.001 acc: 49.599\n",
      "[712] loss: 0.001 acc: 49.599\n",
      "[713] loss: 0.001 acc: 49.599\n",
      "[714] loss: 0.001 acc: 49.599\n",
      "[715] loss: 0.001 acc: 49.599\n",
      "[716] loss: 0.001 acc: 49.599\n",
      "[717] loss: 0.001 acc: 49.599\n",
      "[718] loss: 0.001 acc: 49.599\n",
      "[719] loss: 0.001 acc: 49.599\n",
      "[720] loss: 0.001 acc: 49.599\n",
      "[721] loss: 0.001 acc: 49.599\n",
      "[722] loss: 0.001 acc: 49.599\n",
      "[723] loss: 0.001 acc: 49.599\n",
      "[724] loss: 0.001 acc: 49.599\n",
      "[725] loss: 0.001 acc: 49.599\n",
      "[726] loss: 0.001 acc: 49.599\n",
      "[727] loss: 0.001 acc: 49.599\n",
      "[728] loss: 0.001 acc: 49.599\n",
      "[729] loss: 0.001 acc: 49.599\n",
      "[730] loss: 0.001 acc: 49.599\n",
      "[731] loss: 0.001 acc: 49.599\n",
      "[732] loss: 0.001 acc: 49.599\n",
      "[733] loss: 0.001 acc: 49.599\n",
      "[734] loss: 0.001 acc: 49.599\n",
      "[735] loss: 0.001 acc: 49.599\n",
      "[736] loss: 0.001 acc: 49.599\n",
      "[737] loss: 0.001 acc: 49.599\n",
      "[738] loss: 0.001 acc: 49.599\n",
      "[739] loss: 0.001 acc: 49.599\n",
      "[740] loss: 0.001 acc: 49.599\n",
      "[741] loss: 0.001 acc: 49.599\n",
      "[742] loss: 0.001 acc: 49.599\n",
      "[743] loss: 0.001 acc: 49.599\n",
      "[744] loss: 0.001 acc: 49.599\n",
      "[745] loss: 0.001 acc: 49.599\n",
      "[746] loss: 0.001 acc: 49.599\n",
      "[747] loss: 0.001 acc: 49.599\n",
      "[748] loss: 0.001 acc: 49.599\n",
      "[749] loss: 0.001 acc: 49.599\n",
      "[750] loss: 0.001 acc: 49.599\n",
      "[751] loss: 0.001 acc: 49.599\n",
      "[752] loss: 0.001 acc: 49.599\n",
      "[753] loss: 0.001 acc: 49.599\n",
      "[754] loss: 0.001 acc: 49.599\n",
      "[755] loss: 0.001 acc: 49.599\n",
      "[756] loss: 0.001 acc: 49.599\n",
      "[757] loss: 0.001 acc: 49.599\n",
      "[758] loss: 0.001 acc: 49.599\n",
      "[759] loss: 0.001 acc: 49.599\n",
      "[760] loss: 0.001 acc: 49.599\n",
      "[761] loss: 0.001 acc: 49.599\n",
      "[762] loss: 0.001 acc: 49.599\n",
      "[763] loss: 0.001 acc: 49.599\n",
      "[764] loss: 0.001 acc: 49.599\n",
      "[765] loss: 0.001 acc: 49.599\n",
      "[766] loss: 0.001 acc: 49.599\n",
      "[767] loss: 0.001 acc: 49.599\n",
      "[768] loss: 0.001 acc: 49.599\n",
      "[769] loss: 0.001 acc: 49.599\n",
      "[770] loss: 0.001 acc: 49.599\n",
      "[771] loss: 0.001 acc: 49.599\n",
      "[772] loss: 0.001 acc: 49.599\n",
      "[773] loss: 0.001 acc: 49.599\n",
      "[774] loss: 0.001 acc: 49.599\n",
      "[775] loss: 0.001 acc: 49.599\n",
      "[776] loss: 0.001 acc: 49.599\n",
      "[777] loss: 0.001 acc: 49.599\n",
      "[778] loss: 0.001 acc: 49.599\n",
      "[779] loss: 0.001 acc: 49.599\n",
      "[780] loss: 0.001 acc: 49.599\n",
      "[781] loss: 0.001 acc: 49.599\n",
      "[782] loss: 0.001 acc: 49.599\n",
      "[783] loss: 0.001 acc: 49.599\n",
      "[784] loss: 0.001 acc: 49.599\n",
      "[785] loss: 0.001 acc: 49.599\n",
      "[786] loss: 0.001 acc: 49.599\n",
      "[787] loss: 0.001 acc: 49.599\n",
      "[788] loss: 0.001 acc: 49.599\n",
      "[789] loss: 0.001 acc: 49.599\n",
      "[790] loss: 0.001 acc: 49.599\n",
      "[791] loss: 0.001 acc: 49.599\n",
      "[792] loss: 0.001 acc: 49.599\n",
      "[793] loss: 0.001 acc: 49.599\n",
      "[794] loss: 0.001 acc: 49.599\n",
      "[795] loss: 0.001 acc: 49.599\n",
      "[796] loss: 0.001 acc: 49.599\n",
      "[797] loss: 0.001 acc: 49.599\n",
      "[798] loss: 0.001 acc: 49.599\n",
      "[799] loss: 0.001 acc: 49.599\n",
      "[800] loss: 0.001 acc: 49.599\n",
      "[801] loss: 0.001 acc: 49.599\n",
      "[802] loss: 0.001 acc: 49.599\n",
      "[803] loss: 0.001 acc: 49.599\n",
      "[804] loss: 0.001 acc: 49.599\n",
      "[805] loss: 0.001 acc: 49.599\n",
      "[806] loss: 0.001 acc: 49.599\n",
      "[807] loss: 0.001 acc: 49.599\n",
      "[808] loss: 0.001 acc: 49.599\n",
      "[809] loss: 0.001 acc: 49.599\n",
      "[810] loss: 0.001 acc: 49.599\n",
      "[811] loss: 0.001 acc: 49.599\n",
      "[812] loss: 0.001 acc: 49.599\n",
      "[813] loss: 0.001 acc: 49.599\n",
      "[814] loss: 0.001 acc: 49.599\n",
      "[815] loss: 0.001 acc: 49.599\n",
      "[816] loss: 0.001 acc: 49.599\n",
      "[817] loss: 0.001 acc: 49.599\n",
      "[818] loss: 0.001 acc: 49.599\n",
      "[819] loss: 0.001 acc: 49.599\n",
      "[820] loss: 0.001 acc: 49.599\n",
      "[821] loss: 0.001 acc: 49.599\n",
      "[822] loss: 0.001 acc: 49.599\n",
      "[823] loss: 0.001 acc: 49.599\n",
      "[824] loss: 0.001 acc: 49.599\n",
      "[825] loss: 0.001 acc: 49.599\n",
      "[826] loss: 0.001 acc: 49.599\n",
      "[827] loss: 0.001 acc: 49.599\n",
      "[828] loss: 0.001 acc: 49.599\n",
      "[829] loss: 0.001 acc: 49.599\n",
      "[830] loss: 0.001 acc: 49.599\n",
      "[831] loss: 0.001 acc: 49.599\n",
      "[832] loss: 0.001 acc: 49.599\n",
      "[833] loss: 0.001 acc: 49.599\n",
      "[834] loss: 0.001 acc: 49.599\n",
      "[835] loss: 0.001 acc: 49.599\n",
      "[836] loss: 0.001 acc: 49.584\n",
      "[837] loss: 0.001 acc: 49.599\n",
      "[838] loss: 0.001 acc: 49.584\n",
      "[839] loss: 0.001 acc: 49.599\n",
      "[840] loss: 0.001 acc: 49.584\n",
      "[841] loss: 0.001 acc: 49.599\n",
      "[842] loss: 0.001 acc: 49.584\n",
      "[843] loss: 0.001 acc: 49.599\n",
      "[844] loss: 0.001 acc: 49.584\n",
      "[845] loss: 0.001 acc: 49.599\n",
      "[846] loss: 0.001 acc: 49.584\n",
      "[847] loss: 0.001 acc: 49.599\n",
      "[848] loss: 0.001 acc: 49.568\n",
      "[849] loss: 0.001 acc: 49.615\n",
      "[850] loss: 0.001 acc: 49.584\n",
      "[851] loss: 0.001 acc: 49.630\n",
      "[852] loss: 0.001 acc: 49.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[853] loss: 0.001 acc: 49.645\n",
      "[854] loss: 0.001 acc: 49.537\n",
      "[855] loss: 0.001 acc: 49.630\n",
      "[856] loss: 0.001 acc: 49.568\n",
      "[857] loss: 0.001 acc: 49.584\n",
      "[858] loss: 0.001 acc: 49.584\n",
      "[859] loss: 0.001 acc: 49.553\n",
      "[860] loss: 0.001 acc: 49.645\n",
      "[861] loss: 0.001 acc: 49.599\n",
      "[862] loss: 0.001 acc: 49.630\n",
      "[863] loss: 0.001 acc: 49.599\n",
      "[864] loss: 0.001 acc: 49.630\n",
      "[865] loss: 0.001 acc: 49.507\n",
      "[866] loss: 0.001 acc: 49.584\n",
      "[867] loss: 0.001 acc: 49.198\n",
      "[868] loss: 0.001 acc: 49.306\n",
      "[869] loss: 0.001 acc: 48.181\n",
      "[870] loss: 0.001 acc: 47.872\n",
      "[871] loss: 0.001 acc: 47.872\n",
      "[872] loss: 0.001 acc: 47.549\n",
      "[873] loss: 0.001 acc: 48.952\n",
      "[874] loss: 0.001 acc: 49.553\n",
      "[875] loss: 0.001 acc: 48.689\n",
      "[876] loss: 0.001 acc: 48.551\n",
      "[877] loss: 0.001 acc: 49.615\n",
      "[878] loss: 0.001 acc: 49.337\n",
      "[879] loss: 0.001 acc: 49.337\n",
      "[880] loss: 0.001 acc: 49.707\n",
      "[881] loss: 0.001 acc: 49.322\n",
      "[882] loss: 0.001 acc: 49.692\n",
      "[883] loss: 0.001 acc: 49.553\n",
      "[884] loss: 0.001 acc: 49.661\n",
      "[885] loss: 0.001 acc: 49.245\n",
      "[886] loss: 0.001 acc: 49.522\n",
      "[887] loss: 0.001 acc: 48.057\n",
      "[888] loss: 0.001 acc: 45.575\n",
      "[889] loss: 0.001 acc: 42.677\n",
      "[890] loss: 0.001 acc: 47.903\n",
      "[891] loss: 0.001 acc: 47.857\n",
      "[892] loss: 0.001 acc: 47.148\n",
      "[893] loss: 0.001 acc: 46.315\n",
      "[894] loss: 0.001 acc: 48.289\n",
      "[895] loss: 0.001 acc: 45.236\n",
      "[896] loss: 0.001 acc: 40.518\n",
      "[897] loss: 0.001 acc: 44.342\n",
      "[898] loss: 0.001 acc: 39.331\n",
      "[899] loss: 0.001 acc: 36.633\n",
      "[900] loss: 0.001 acc: 39.778\n",
      "[901] loss: 0.001 acc: 47.055\n",
      "[902] loss: 0.001 acc: 43.494\n",
      "[903] loss: 0.001 acc: 44.619\n",
      "[904] loss: 0.001 acc: 46.577\n",
      "[905] loss: 0.001 acc: 41.058\n",
      "[906] loss: 0.001 acc: 40.611\n",
      "[907] loss: 0.001 acc: 42.831\n",
      "[908] loss: 0.001 acc: 33.087\n",
      "[909] loss: 0.001 acc: 44.126\n",
      "[910] loss: 0.001 acc: 40.657\n",
      "[911] loss: 0.001 acc: 35.060\n",
      "[912] loss: 0.001 acc: 44.372\n",
      "[913] loss: 0.001 acc: 35.029\n",
      "[914] loss: 0.001 acc: 35.368\n",
      "[915] loss: 0.001 acc: 40.765\n",
      "[916] loss: 0.001 acc: 38.020\n",
      "[917] loss: 0.001 acc: 32.562\n",
      "[918] loss: 0.001 acc: 34.983\n",
      "[919] loss: 0.002 acc: 35.908\n",
      "[920] loss: 0.002 acc: 31.869\n",
      "[921] loss: 0.001 acc: 32.794\n",
      "[922] loss: 0.004 acc: 22.109\n",
      "[923] loss: 0.004 acc: 30.574\n",
      "[924] loss: 0.005 acc: 30.851\n",
      "[925] loss: 0.009 acc: 35.446\n",
      "[926] loss: 0.008 acc: 36.139\n",
      "[927] loss: 0.006 acc: 26.364\n",
      "[928] loss: 0.010 acc: 31.267\n",
      "[929] loss: 0.006 acc: 38.853\n",
      "[930] loss: 0.007 acc: 32.516\n",
      "[931] loss: 0.007 acc: 37.265\n",
      "[932] loss: 0.008 acc: 26.827\n",
      "[933] loss: 0.009 acc: 35.153\n",
      "[934] loss: 0.005 acc: 41.644\n",
      "[935] loss: 0.006 acc: 36.756\n",
      "[936] loss: 0.004 acc: 35.893\n",
      "[937] loss: 0.010 acc: 32.624\n",
      "[938] loss: 0.007 acc: 29.448\n",
      "[939] loss: 0.004 acc: 41.397\n",
      "[940] loss: 0.005 acc: 34.474\n",
      "[941] loss: 0.003 acc: 33.195\n",
      "[942] loss: 0.003 acc: 30.419\n",
      "[943] loss: 0.002 acc: 30.728\n",
      "[944] loss: 0.003 acc: 26.303\n",
      "[945] loss: 0.003 acc: 27.320\n",
      "[946] loss: 0.006 acc: 32.485\n",
      "[947] loss: 0.008 acc: 31.699\n",
      "[948] loss: 0.004 acc: 40.641\n",
      "[949] loss: 0.005 acc: 43.602\n",
      "[950] loss: 0.004 acc: 34.767\n",
      "[951] loss: 0.003 acc: 34.891\n",
      "[952] loss: 0.005 acc: 32.886\n",
      "[953] loss: 0.003 acc: 29.032\n",
      "[954] loss: 0.004 acc: 26.765\n",
      "[955] loss: 0.008 acc: 25.301\n",
      "[956] loss: 0.003 acc: 36.340\n",
      "[957] loss: 0.003 acc: 42.044\n",
      "[958] loss: 0.002 acc: 43.494\n",
      "[959] loss: 0.006 acc: 33.580\n",
      "[960] loss: 0.002 acc: 28.446\n",
      "[961] loss: 0.007 acc: 29.124\n",
      "[962] loss: 0.014 acc: 24.607\n",
      "[963] loss: 0.010 acc: 27.274\n",
      "[964] loss: 0.011 acc: 34.135\n",
      "[965] loss: 0.011 acc: 42.353\n",
      "[966] loss: 0.010 acc: 41.088\n",
      "[967] loss: 0.006 acc: 41.042\n",
      "[968] loss: 0.008 acc: 28.770\n",
      "[969] loss: 0.009 acc: 27.552\n",
      "[970] loss: 0.005 acc: 34.628\n",
      "[971] loss: 0.004 acc: 37.912\n",
      "[972] loss: 0.005 acc: 38.699\n",
      "[973] loss: 0.004 acc: 37.635\n",
      "[974] loss: 0.004 acc: 28.693\n",
      "[975] loss: 0.003 acc: 35.399\n",
      "[976] loss: 0.004 acc: 21.554\n",
      "[977] loss: 0.003 acc: 28.816\n",
      "[978] loss: 0.003 acc: 33.673\n",
      "[979] loss: 0.004 acc: 36.016\n",
      "[980] loss: 0.004 acc: 41.289\n",
      "[981] loss: 0.003 acc: 39.315\n",
      "[982] loss: 0.003 acc: 38.822\n",
      "[983] loss: 0.003 acc: 34.659\n",
      "[984] loss: 0.003 acc: 26.935\n",
      "[985] loss: 0.002 acc: 30.096\n",
      "[986] loss: 0.003 acc: 33.410\n",
      "[987] loss: 0.002 acc: 40.826\n",
      "[988] loss: 0.002 acc: 43.648\n",
      "[989] loss: 0.002 acc: 39.531\n",
      "[990] loss: 0.002 acc: 37.111\n",
      "[991] loss: 0.001 acc: 37.928\n",
      "[992] loss: 0.002 acc: 34.628\n",
      "[993] loss: 0.001 acc: 33.734\n",
      "[994] loss: 0.001 acc: 29.417\n",
      "[995] loss: 0.001 acc: 30.635\n",
      "[996] loss: 0.001 acc: 31.699\n",
      "[997] loss: 0.001 acc: 33.179\n",
      "[998] loss: 0.001 acc: 36.278\n",
      "[999] loss: 0.001 acc: 32.023\n",
      "[1000] loss: 0.001 acc: 42.661\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(X_traintorch.float())\n",
    "    loss = criterion(outputs, y_traintorch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_traintorch.size(0)\n",
    "    correct += (predicted == y_traintorch).sum().item()\n",
    "    \n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f acc: %.3f' %\n",
    "              (epoch + 1, running_loss / 2000, (100 * correct / total)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_valtorch.float())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_valtorch.size(0)\n",
    "    correct += (predicted == y_valtorch).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:08:59] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.49827856025039124\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5, silence=False)\n",
    "XGB.fit(X_train, y_train)\n",
    "y_pred = XGB.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, np.around(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 3., ..., 1., 1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4682316118935837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "reg=LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred=reg.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val,y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43380281690140843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DTree=DecisionTreeRegressor()\n",
    "DTree.fit(X_train,y_train)\n",
    "y_pred=DTree.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val,y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 6486, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 2.491520\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "param = {'num_leaves':150, 'max_depth':7,'learning_rate':.05,'max_bin':200}\n",
    "\n",
    "val_data=lgb.Dataset(X_val,label=y_val)\n",
    "clf = lgb.train(param,train_data,100)\n",
    "y_pred=clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49827856025039124\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val,np.around(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=3, min_child_weight=2, n_estimators=50\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[00:12:04] C:/Jenkins/workspace/xgboost-win64_release_0.90/src/metric/elementwise_metric.cu:326: Check failed: preds.Size() == info.labels_.Size() (38720 vs. 7744) : label and prediction size not match, hint: use merror or mlogloss for multi-class classification",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-afadc9ebdbde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mnfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     )\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Update best MAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, iteration, feval)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;34m\"\"\"\"Evaluate the CVPack for one iteration.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36meval_set\u001b[1;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[0;32m   1170\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m                                               \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[0;32m   1173\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \"\"\"\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [00:12:04] C:/Jenkins/workspace/xgboost-win64_release_0.90/src/metric/elementwise_metric.cu:326: Check failed: preds.Size() == info.labels_.Size() (38720 vs. 7744) : label and prediction size not match, hint: use merror or mlogloss for multi-class classification"
     ]
    }
   ],
   "source": [
    "# Define initial best params and MAE\n",
    "params = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 5} \n",
    "\n",
    "dvals = xgb.DMatrix(X, label=y)\n",
    "\n",
    "n_estimators = range(50, 400, 25)\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight,n_estimators)\n",
    "    for max_depth in range(3,20)\n",
    "    for min_child_weight in range(2,15)\n",
    "    for n_estimators in range(50,400,25)\n",
    "]\n",
    "for max_depth, min_child_weight, n_estimators in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}, n_estimators={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight, n_estimators))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['n_estimators'] = n_estimators\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dvals,\n",
    "        num_boost_round=999,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {},, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 10,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 5}  # the number of classes that exist in this datset\n",
    "num_round = 100  # the number of training iterations\n",
    "params['max_depth'] = 4\n",
    "params['min_child_weight'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540455695326385\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_val, best_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5311424100156494\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, best_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6933    1\n",
       "7012    1\n",
       "5709    3\n",
       "4453    2\n",
       "7395    3\n",
       "       ..\n",
       "9073    2\n",
       "3150    3\n",
       "5227    1\n",
       "5573    1\n",
       "3497    3\n",
       "Name: price, Length: 3195, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight, n_estimators)\n",
    "    for max_depth in range(3,20)\n",
    "    for min_child_weight in range(2,15)\n",
    "    for n_estimators in range(50, 400, 25)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2, 50),\n",
       " (3, 2, 75),\n",
       " (3, 2, 100),\n",
       " (3, 2, 125),\n",
       " (3, 2, 150),\n",
       " (3, 2, 175),\n",
       " (3, 2, 200),\n",
       " (3, 2, 225),\n",
       " (3, 2, 250),\n",
       " (3, 2, 275),\n",
       " (3, 2, 300),\n",
       " (3, 2, 325),\n",
       " (3, 2, 350),\n",
       " (3, 2, 375),\n",
       " (3, 3, 50),\n",
       " (3, 3, 75),\n",
       " (3, 3, 100),\n",
       " (3, 3, 125),\n",
       " (3, 3, 150),\n",
       " (3, 3, 175),\n",
       " (3, 3, 200),\n",
       " (3, 3, 225),\n",
       " (3, 3, 250),\n",
       " (3, 3, 275),\n",
       " (3, 3, 300),\n",
       " (3, 3, 325),\n",
       " (3, 3, 350),\n",
       " (3, 3, 375),\n",
       " (3, 4, 50),\n",
       " (3, 4, 75),\n",
       " (3, 4, 100),\n",
       " (3, 4, 125),\n",
       " (3, 4, 150),\n",
       " (3, 4, 175),\n",
       " (3, 4, 200),\n",
       " (3, 4, 225),\n",
       " (3, 4, 250),\n",
       " (3, 4, 275),\n",
       " (3, 4, 300),\n",
       " (3, 4, 325),\n",
       " (3, 4, 350),\n",
       " (3, 4, 375),\n",
       " (3, 5, 50),\n",
       " (3, 5, 75),\n",
       " (3, 5, 100),\n",
       " (3, 5, 125),\n",
       " (3, 5, 150),\n",
       " (3, 5, 175),\n",
       " (3, 5, 200),\n",
       " (3, 5, 225),\n",
       " (3, 5, 250),\n",
       " (3, 5, 275),\n",
       " (3, 5, 300),\n",
       " (3, 5, 325),\n",
       " (3, 5, 350),\n",
       " (3, 5, 375),\n",
       " (3, 6, 50),\n",
       " (3, 6, 75),\n",
       " (3, 6, 100),\n",
       " (3, 6, 125),\n",
       " (3, 6, 150),\n",
       " (3, 6, 175),\n",
       " (3, 6, 200),\n",
       " (3, 6, 225),\n",
       " (3, 6, 250),\n",
       " (3, 6, 275),\n",
       " (3, 6, 300),\n",
       " (3, 6, 325),\n",
       " (3, 6, 350),\n",
       " (3, 6, 375),\n",
       " (3, 7, 50),\n",
       " (3, 7, 75),\n",
       " (3, 7, 100),\n",
       " (3, 7, 125),\n",
       " (3, 7, 150),\n",
       " (3, 7, 175),\n",
       " (3, 7, 200),\n",
       " (3, 7, 225),\n",
       " (3, 7, 250),\n",
       " (3, 7, 275),\n",
       " (3, 7, 300),\n",
       " (3, 7, 325),\n",
       " (3, 7, 350),\n",
       " (3, 7, 375),\n",
       " (3, 8, 50),\n",
       " (3, 8, 75),\n",
       " (3, 8, 100),\n",
       " (3, 8, 125),\n",
       " (3, 8, 150),\n",
       " (3, 8, 175),\n",
       " (3, 8, 200),\n",
       " (3, 8, 225),\n",
       " (3, 8, 250),\n",
       " (3, 8, 275),\n",
       " (3, 8, 300),\n",
       " (3, 8, 325),\n",
       " (3, 8, 350),\n",
       " (3, 8, 375),\n",
       " (3, 9, 50),\n",
       " (3, 9, 75),\n",
       " (3, 9, 100),\n",
       " (3, 9, 125),\n",
       " (3, 9, 150),\n",
       " (3, 9, 175),\n",
       " (3, 9, 200),\n",
       " (3, 9, 225),\n",
       " (3, 9, 250),\n",
       " (3, 9, 275),\n",
       " (3, 9, 300),\n",
       " (3, 9, 325),\n",
       " (3, 9, 350),\n",
       " (3, 9, 375),\n",
       " (3, 10, 50),\n",
       " (3, 10, 75),\n",
       " (3, 10, 100),\n",
       " (3, 10, 125),\n",
       " (3, 10, 150),\n",
       " (3, 10, 175),\n",
       " (3, 10, 200),\n",
       " (3, 10, 225),\n",
       " (3, 10, 250),\n",
       " (3, 10, 275),\n",
       " (3, 10, 300),\n",
       " (3, 10, 325),\n",
       " (3, 10, 350),\n",
       " (3, 10, 375),\n",
       " (3, 11, 50),\n",
       " (3, 11, 75),\n",
       " (3, 11, 100),\n",
       " (3, 11, 125),\n",
       " (3, 11, 150),\n",
       " (3, 11, 175),\n",
       " (3, 11, 200),\n",
       " (3, 11, 225),\n",
       " (3, 11, 250),\n",
       " (3, 11, 275),\n",
       " (3, 11, 300),\n",
       " (3, 11, 325),\n",
       " (3, 11, 350),\n",
       " (3, 11, 375),\n",
       " (3, 12, 50),\n",
       " (3, 12, 75),\n",
       " (3, 12, 100),\n",
       " (3, 12, 125),\n",
       " (3, 12, 150),\n",
       " (3, 12, 175),\n",
       " (3, 12, 200),\n",
       " (3, 12, 225),\n",
       " (3, 12, 250),\n",
       " (3, 12, 275),\n",
       " (3, 12, 300),\n",
       " (3, 12, 325),\n",
       " (3, 12, 350),\n",
       " (3, 12, 375),\n",
       " (3, 13, 50),\n",
       " (3, 13, 75),\n",
       " (3, 13, 100),\n",
       " (3, 13, 125),\n",
       " (3, 13, 150),\n",
       " (3, 13, 175),\n",
       " (3, 13, 200),\n",
       " (3, 13, 225),\n",
       " (3, 13, 250),\n",
       " (3, 13, 275),\n",
       " (3, 13, 300),\n",
       " (3, 13, 325),\n",
       " (3, 13, 350),\n",
       " (3, 13, 375),\n",
       " (3, 14, 50),\n",
       " (3, 14, 75),\n",
       " (3, 14, 100),\n",
       " (3, 14, 125),\n",
       " (3, 14, 150),\n",
       " (3, 14, 175),\n",
       " (3, 14, 200),\n",
       " (3, 14, 225),\n",
       " (3, 14, 250),\n",
       " (3, 14, 275),\n",
       " (3, 14, 300),\n",
       " (3, 14, 325),\n",
       " (3, 14, 350),\n",
       " (3, 14, 375),\n",
       " (4, 2, 50),\n",
       " (4, 2, 75),\n",
       " (4, 2, 100),\n",
       " (4, 2, 125),\n",
       " (4, 2, 150),\n",
       " (4, 2, 175),\n",
       " (4, 2, 200),\n",
       " (4, 2, 225),\n",
       " (4, 2, 250),\n",
       " (4, 2, 275),\n",
       " (4, 2, 300),\n",
       " (4, 2, 325),\n",
       " (4, 2, 350),\n",
       " (4, 2, 375),\n",
       " (4, 3, 50),\n",
       " (4, 3, 75),\n",
       " (4, 3, 100),\n",
       " (4, 3, 125),\n",
       " (4, 3, 150),\n",
       " (4, 3, 175),\n",
       " (4, 3, 200),\n",
       " (4, 3, 225),\n",
       " (4, 3, 250),\n",
       " (4, 3, 275),\n",
       " (4, 3, 300),\n",
       " (4, 3, 325),\n",
       " (4, 3, 350),\n",
       " (4, 3, 375),\n",
       " (4, 4, 50),\n",
       " (4, 4, 75),\n",
       " (4, 4, 100),\n",
       " (4, 4, 125),\n",
       " (4, 4, 150),\n",
       " (4, 4, 175),\n",
       " (4, 4, 200),\n",
       " (4, 4, 225),\n",
       " (4, 4, 250),\n",
       " (4, 4, 275),\n",
       " (4, 4, 300),\n",
       " (4, 4, 325),\n",
       " (4, 4, 350),\n",
       " (4, 4, 375),\n",
       " (4, 5, 50),\n",
       " (4, 5, 75),\n",
       " (4, 5, 100),\n",
       " (4, 5, 125),\n",
       " (4, 5, 150),\n",
       " (4, 5, 175),\n",
       " (4, 5, 200),\n",
       " (4, 5, 225),\n",
       " (4, 5, 250),\n",
       " (4, 5, 275),\n",
       " (4, 5, 300),\n",
       " (4, 5, 325),\n",
       " (4, 5, 350),\n",
       " (4, 5, 375),\n",
       " (4, 6, 50),\n",
       " (4, 6, 75),\n",
       " (4, 6, 100),\n",
       " (4, 6, 125),\n",
       " (4, 6, 150),\n",
       " (4, 6, 175),\n",
       " (4, 6, 200),\n",
       " (4, 6, 225),\n",
       " (4, 6, 250),\n",
       " (4, 6, 275),\n",
       " (4, 6, 300),\n",
       " (4, 6, 325),\n",
       " (4, 6, 350),\n",
       " (4, 6, 375),\n",
       " (4, 7, 50),\n",
       " (4, 7, 75),\n",
       " (4, 7, 100),\n",
       " (4, 7, 125),\n",
       " (4, 7, 150),\n",
       " (4, 7, 175),\n",
       " (4, 7, 200),\n",
       " (4, 7, 225),\n",
       " (4, 7, 250),\n",
       " (4, 7, 275),\n",
       " (4, 7, 300),\n",
       " (4, 7, 325),\n",
       " (4, 7, 350),\n",
       " (4, 7, 375),\n",
       " (4, 8, 50),\n",
       " (4, 8, 75),\n",
       " (4, 8, 100),\n",
       " (4, 8, 125),\n",
       " (4, 8, 150),\n",
       " (4, 8, 175),\n",
       " (4, 8, 200),\n",
       " (4, 8, 225),\n",
       " (4, 8, 250),\n",
       " (4, 8, 275),\n",
       " (4, 8, 300),\n",
       " (4, 8, 325),\n",
       " (4, 8, 350),\n",
       " (4, 8, 375),\n",
       " (4, 9, 50),\n",
       " (4, 9, 75),\n",
       " (4, 9, 100),\n",
       " (4, 9, 125),\n",
       " (4, 9, 150),\n",
       " (4, 9, 175),\n",
       " (4, 9, 200),\n",
       " (4, 9, 225),\n",
       " (4, 9, 250),\n",
       " (4, 9, 275),\n",
       " (4, 9, 300),\n",
       " (4, 9, 325),\n",
       " (4, 9, 350),\n",
       " (4, 9, 375),\n",
       " (4, 10, 50),\n",
       " (4, 10, 75),\n",
       " (4, 10, 100),\n",
       " (4, 10, 125),\n",
       " (4, 10, 150),\n",
       " (4, 10, 175),\n",
       " (4, 10, 200),\n",
       " (4, 10, 225),\n",
       " (4, 10, 250),\n",
       " (4, 10, 275),\n",
       " (4, 10, 300),\n",
       " (4, 10, 325),\n",
       " (4, 10, 350),\n",
       " (4, 10, 375),\n",
       " (4, 11, 50),\n",
       " (4, 11, 75),\n",
       " (4, 11, 100),\n",
       " (4, 11, 125),\n",
       " (4, 11, 150),\n",
       " (4, 11, 175),\n",
       " (4, 11, 200),\n",
       " (4, 11, 225),\n",
       " (4, 11, 250),\n",
       " (4, 11, 275),\n",
       " (4, 11, 300),\n",
       " (4, 11, 325),\n",
       " (4, 11, 350),\n",
       " (4, 11, 375),\n",
       " (4, 12, 50),\n",
       " (4, 12, 75),\n",
       " (4, 12, 100),\n",
       " (4, 12, 125),\n",
       " (4, 12, 150),\n",
       " (4, 12, 175),\n",
       " (4, 12, 200),\n",
       " (4, 12, 225),\n",
       " (4, 12, 250),\n",
       " (4, 12, 275),\n",
       " (4, 12, 300),\n",
       " (4, 12, 325),\n",
       " (4, 12, 350),\n",
       " (4, 12, 375),\n",
       " (4, 13, 50),\n",
       " (4, 13, 75),\n",
       " (4, 13, 100),\n",
       " (4, 13, 125),\n",
       " (4, 13, 150),\n",
       " (4, 13, 175),\n",
       " (4, 13, 200),\n",
       " (4, 13, 225),\n",
       " (4, 13, 250),\n",
       " (4, 13, 275),\n",
       " (4, 13, 300),\n",
       " (4, 13, 325),\n",
       " (4, 13, 350),\n",
       " (4, 13, 375),\n",
       " (4, 14, 50),\n",
       " (4, 14, 75),\n",
       " (4, 14, 100),\n",
       " (4, 14, 125),\n",
       " (4, 14, 150),\n",
       " (4, 14, 175),\n",
       " (4, 14, 200),\n",
       " (4, 14, 225),\n",
       " (4, 14, 250),\n",
       " (4, 14, 275),\n",
       " (4, 14, 300),\n",
       " (4, 14, 325),\n",
       " (4, 14, 350),\n",
       " (4, 14, 375),\n",
       " (5, 2, 50),\n",
       " (5, 2, 75),\n",
       " (5, 2, 100),\n",
       " (5, 2, 125),\n",
       " (5, 2, 150),\n",
       " (5, 2, 175),\n",
       " (5, 2, 200),\n",
       " (5, 2, 225),\n",
       " (5, 2, 250),\n",
       " (5, 2, 275),\n",
       " (5, 2, 300),\n",
       " (5, 2, 325),\n",
       " (5, 2, 350),\n",
       " (5, 2, 375),\n",
       " (5, 3, 50),\n",
       " (5, 3, 75),\n",
       " (5, 3, 100),\n",
       " (5, 3, 125),\n",
       " (5, 3, 150),\n",
       " (5, 3, 175),\n",
       " (5, 3, 200),\n",
       " (5, 3, 225),\n",
       " (5, 3, 250),\n",
       " (5, 3, 275),\n",
       " (5, 3, 300),\n",
       " (5, 3, 325),\n",
       " (5, 3, 350),\n",
       " (5, 3, 375),\n",
       " (5, 4, 50),\n",
       " (5, 4, 75),\n",
       " (5, 4, 100),\n",
       " (5, 4, 125),\n",
       " (5, 4, 150),\n",
       " (5, 4, 175),\n",
       " (5, 4, 200),\n",
       " (5, 4, 225),\n",
       " (5, 4, 250),\n",
       " (5, 4, 275),\n",
       " (5, 4, 300),\n",
       " (5, 4, 325),\n",
       " (5, 4, 350),\n",
       " (5, 4, 375),\n",
       " (5, 5, 50),\n",
       " (5, 5, 75),\n",
       " (5, 5, 100),\n",
       " (5, 5, 125),\n",
       " (5, 5, 150),\n",
       " (5, 5, 175),\n",
       " (5, 5, 200),\n",
       " (5, 5, 225),\n",
       " (5, 5, 250),\n",
       " (5, 5, 275),\n",
       " (5, 5, 300),\n",
       " (5, 5, 325),\n",
       " (5, 5, 350),\n",
       " (5, 5, 375),\n",
       " (5, 6, 50),\n",
       " (5, 6, 75),\n",
       " (5, 6, 100),\n",
       " (5, 6, 125),\n",
       " (5, 6, 150),\n",
       " (5, 6, 175),\n",
       " (5, 6, 200),\n",
       " (5, 6, 225),\n",
       " (5, 6, 250),\n",
       " (5, 6, 275),\n",
       " (5, 6, 300),\n",
       " (5, 6, 325),\n",
       " (5, 6, 350),\n",
       " (5, 6, 375),\n",
       " (5, 7, 50),\n",
       " (5, 7, 75),\n",
       " (5, 7, 100),\n",
       " (5, 7, 125),\n",
       " (5, 7, 150),\n",
       " (5, 7, 175),\n",
       " (5, 7, 200),\n",
       " (5, 7, 225),\n",
       " (5, 7, 250),\n",
       " (5, 7, 275),\n",
       " (5, 7, 300),\n",
       " (5, 7, 325),\n",
       " (5, 7, 350),\n",
       " (5, 7, 375),\n",
       " (5, 8, 50),\n",
       " (5, 8, 75),\n",
       " (5, 8, 100),\n",
       " (5, 8, 125),\n",
       " (5, 8, 150),\n",
       " (5, 8, 175),\n",
       " (5, 8, 200),\n",
       " (5, 8, 225),\n",
       " (5, 8, 250),\n",
       " (5, 8, 275),\n",
       " (5, 8, 300),\n",
       " (5, 8, 325),\n",
       " (5, 8, 350),\n",
       " (5, 8, 375),\n",
       " (5, 9, 50),\n",
       " (5, 9, 75),\n",
       " (5, 9, 100),\n",
       " (5, 9, 125),\n",
       " (5, 9, 150),\n",
       " (5, 9, 175),\n",
       " (5, 9, 200),\n",
       " (5, 9, 225),\n",
       " (5, 9, 250),\n",
       " (5, 9, 275),\n",
       " (5, 9, 300),\n",
       " (5, 9, 325),\n",
       " (5, 9, 350),\n",
       " (5, 9, 375),\n",
       " (5, 10, 50),\n",
       " (5, 10, 75),\n",
       " (5, 10, 100),\n",
       " (5, 10, 125),\n",
       " (5, 10, 150),\n",
       " (5, 10, 175),\n",
       " (5, 10, 200),\n",
       " (5, 10, 225),\n",
       " (5, 10, 250),\n",
       " (5, 10, 275),\n",
       " (5, 10, 300),\n",
       " (5, 10, 325),\n",
       " (5, 10, 350),\n",
       " (5, 10, 375),\n",
       " (5, 11, 50),\n",
       " (5, 11, 75),\n",
       " (5, 11, 100),\n",
       " (5, 11, 125),\n",
       " (5, 11, 150),\n",
       " (5, 11, 175),\n",
       " (5, 11, 200),\n",
       " (5, 11, 225),\n",
       " (5, 11, 250),\n",
       " (5, 11, 275),\n",
       " (5, 11, 300),\n",
       " (5, 11, 325),\n",
       " (5, 11, 350),\n",
       " (5, 11, 375),\n",
       " (5, 12, 50),\n",
       " (5, 12, 75),\n",
       " (5, 12, 100),\n",
       " (5, 12, 125),\n",
       " (5, 12, 150),\n",
       " (5, 12, 175),\n",
       " (5, 12, 200),\n",
       " (5, 12, 225),\n",
       " (5, 12, 250),\n",
       " (5, 12, 275),\n",
       " (5, 12, 300),\n",
       " (5, 12, 325),\n",
       " (5, 12, 350),\n",
       " (5, 12, 375),\n",
       " (5, 13, 50),\n",
       " (5, 13, 75),\n",
       " (5, 13, 100),\n",
       " (5, 13, 125),\n",
       " (5, 13, 150),\n",
       " (5, 13, 175),\n",
       " (5, 13, 200),\n",
       " (5, 13, 225),\n",
       " (5, 13, 250),\n",
       " (5, 13, 275),\n",
       " (5, 13, 300),\n",
       " (5, 13, 325),\n",
       " (5, 13, 350),\n",
       " (5, 13, 375),\n",
       " (5, 14, 50),\n",
       " (5, 14, 75),\n",
       " (5, 14, 100),\n",
       " (5, 14, 125),\n",
       " (5, 14, 150),\n",
       " (5, 14, 175),\n",
       " (5, 14, 200),\n",
       " (5, 14, 225),\n",
       " (5, 14, 250),\n",
       " (5, 14, 275),\n",
       " (5, 14, 300),\n",
       " (5, 14, 325),\n",
       " (5, 14, 350),\n",
       " (5, 14, 375),\n",
       " (6, 2, 50),\n",
       " (6, 2, 75),\n",
       " (6, 2, 100),\n",
       " (6, 2, 125),\n",
       " (6, 2, 150),\n",
       " (6, 2, 175),\n",
       " (6, 2, 200),\n",
       " (6, 2, 225),\n",
       " (6, 2, 250),\n",
       " (6, 2, 275),\n",
       " (6, 2, 300),\n",
       " (6, 2, 325),\n",
       " (6, 2, 350),\n",
       " (6, 2, 375),\n",
       " (6, 3, 50),\n",
       " (6, 3, 75),\n",
       " (6, 3, 100),\n",
       " (6, 3, 125),\n",
       " (6, 3, 150),\n",
       " (6, 3, 175),\n",
       " (6, 3, 200),\n",
       " (6, 3, 225),\n",
       " (6, 3, 250),\n",
       " (6, 3, 275),\n",
       " (6, 3, 300),\n",
       " (6, 3, 325),\n",
       " (6, 3, 350),\n",
       " (6, 3, 375),\n",
       " (6, 4, 50),\n",
       " (6, 4, 75),\n",
       " (6, 4, 100),\n",
       " (6, 4, 125),\n",
       " (6, 4, 150),\n",
       " (6, 4, 175),\n",
       " (6, 4, 200),\n",
       " (6, 4, 225),\n",
       " (6, 4, 250),\n",
       " (6, 4, 275),\n",
       " (6, 4, 300),\n",
       " (6, 4, 325),\n",
       " (6, 4, 350),\n",
       " (6, 4, 375),\n",
       " (6, 5, 50),\n",
       " (6, 5, 75),\n",
       " (6, 5, 100),\n",
       " (6, 5, 125),\n",
       " (6, 5, 150),\n",
       " (6, 5, 175),\n",
       " (6, 5, 200),\n",
       " (6, 5, 225),\n",
       " (6, 5, 250),\n",
       " (6, 5, 275),\n",
       " (6, 5, 300),\n",
       " (6, 5, 325),\n",
       " (6, 5, 350),\n",
       " (6, 5, 375),\n",
       " (6, 6, 50),\n",
       " (6, 6, 75),\n",
       " (6, 6, 100),\n",
       " (6, 6, 125),\n",
       " (6, 6, 150),\n",
       " (6, 6, 175),\n",
       " (6, 6, 200),\n",
       " (6, 6, 225),\n",
       " (6, 6, 250),\n",
       " (6, 6, 275),\n",
       " (6, 6, 300),\n",
       " (6, 6, 325),\n",
       " (6, 6, 350),\n",
       " (6, 6, 375),\n",
       " (6, 7, 50),\n",
       " (6, 7, 75),\n",
       " (6, 7, 100),\n",
       " (6, 7, 125),\n",
       " (6, 7, 150),\n",
       " (6, 7, 175),\n",
       " (6, 7, 200),\n",
       " (6, 7, 225),\n",
       " (6, 7, 250),\n",
       " (6, 7, 275),\n",
       " (6, 7, 300),\n",
       " (6, 7, 325),\n",
       " (6, 7, 350),\n",
       " (6, 7, 375),\n",
       " (6, 8, 50),\n",
       " (6, 8, 75),\n",
       " (6, 8, 100),\n",
       " (6, 8, 125),\n",
       " (6, 8, 150),\n",
       " (6, 8, 175),\n",
       " (6, 8, 200),\n",
       " (6, 8, 225),\n",
       " (6, 8, 250),\n",
       " (6, 8, 275),\n",
       " (6, 8, 300),\n",
       " (6, 8, 325),\n",
       " (6, 8, 350),\n",
       " (6, 8, 375),\n",
       " (6, 9, 50),\n",
       " (6, 9, 75),\n",
       " (6, 9, 100),\n",
       " (6, 9, 125),\n",
       " (6, 9, 150),\n",
       " (6, 9, 175),\n",
       " (6, 9, 200),\n",
       " (6, 9, 225),\n",
       " (6, 9, 250),\n",
       " (6, 9, 275),\n",
       " (6, 9, 300),\n",
       " (6, 9, 325),\n",
       " (6, 9, 350),\n",
       " (6, 9, 375),\n",
       " (6, 10, 50),\n",
       " (6, 10, 75),\n",
       " (6, 10, 100),\n",
       " (6, 10, 125),\n",
       " (6, 10, 150),\n",
       " (6, 10, 175),\n",
       " (6, 10, 200),\n",
       " (6, 10, 225),\n",
       " (6, 10, 250),\n",
       " (6, 10, 275),\n",
       " (6, 10, 300),\n",
       " (6, 10, 325),\n",
       " (6, 10, 350),\n",
       " (6, 10, 375),\n",
       " (6, 11, 50),\n",
       " (6, 11, 75),\n",
       " (6, 11, 100),\n",
       " (6, 11, 125),\n",
       " (6, 11, 150),\n",
       " (6, 11, 175),\n",
       " (6, 11, 200),\n",
       " (6, 11, 225),\n",
       " (6, 11, 250),\n",
       " (6, 11, 275),\n",
       " (6, 11, 300),\n",
       " (6, 11, 325),\n",
       " (6, 11, 350),\n",
       " (6, 11, 375),\n",
       " (6, 12, 50),\n",
       " (6, 12, 75),\n",
       " (6, 12, 100),\n",
       " (6, 12, 125),\n",
       " (6, 12, 150),\n",
       " (6, 12, 175),\n",
       " (6, 12, 200),\n",
       " (6, 12, 225),\n",
       " (6, 12, 250),\n",
       " (6, 12, 275),\n",
       " (6, 12, 300),\n",
       " (6, 12, 325),\n",
       " (6, 12, 350),\n",
       " (6, 12, 375),\n",
       " (6, 13, 50),\n",
       " (6, 13, 75),\n",
       " (6, 13, 100),\n",
       " (6, 13, 125),\n",
       " (6, 13, 150),\n",
       " (6, 13, 175),\n",
       " (6, 13, 200),\n",
       " (6, 13, 225),\n",
       " (6, 13, 250),\n",
       " (6, 13, 275),\n",
       " (6, 13, 300),\n",
       " (6, 13, 325),\n",
       " (6, 13, 350),\n",
       " (6, 13, 375),\n",
       " (6, 14, 50),\n",
       " (6, 14, 75),\n",
       " (6, 14, 100),\n",
       " (6, 14, 125),\n",
       " (6, 14, 150),\n",
       " (6, 14, 175),\n",
       " (6, 14, 200),\n",
       " (6, 14, 225),\n",
       " (6, 14, 250),\n",
       " (6, 14, 275),\n",
       " (6, 14, 300),\n",
       " (6, 14, 325),\n",
       " (6, 14, 350),\n",
       " (6, 14, 375),\n",
       " (7, 2, 50),\n",
       " (7, 2, 75),\n",
       " (7, 2, 100),\n",
       " (7, 2, 125),\n",
       " (7, 2, 150),\n",
       " (7, 2, 175),\n",
       " (7, 2, 200),\n",
       " (7, 2, 225),\n",
       " (7, 2, 250),\n",
       " (7, 2, 275),\n",
       " (7, 2, 300),\n",
       " (7, 2, 325),\n",
       " (7, 2, 350),\n",
       " (7, 2, 375),\n",
       " (7, 3, 50),\n",
       " (7, 3, 75),\n",
       " (7, 3, 100),\n",
       " (7, 3, 125),\n",
       " (7, 3, 150),\n",
       " (7, 3, 175),\n",
       " (7, 3, 200),\n",
       " (7, 3, 225),\n",
       " (7, 3, 250),\n",
       " (7, 3, 275),\n",
       " (7, 3, 300),\n",
       " (7, 3, 325),\n",
       " (7, 3, 350),\n",
       " (7, 3, 375),\n",
       " (7, 4, 50),\n",
       " (7, 4, 75),\n",
       " (7, 4, 100),\n",
       " (7, 4, 125),\n",
       " (7, 4, 150),\n",
       " (7, 4, 175),\n",
       " (7, 4, 200),\n",
       " (7, 4, 225),\n",
       " (7, 4, 250),\n",
       " (7, 4, 275),\n",
       " (7, 4, 300),\n",
       " (7, 4, 325),\n",
       " (7, 4, 350),\n",
       " (7, 4, 375),\n",
       " (7, 5, 50),\n",
       " (7, 5, 75),\n",
       " (7, 5, 100),\n",
       " (7, 5, 125),\n",
       " (7, 5, 150),\n",
       " (7, 5, 175),\n",
       " (7, 5, 200),\n",
       " (7, 5, 225),\n",
       " (7, 5, 250),\n",
       " (7, 5, 275),\n",
       " (7, 5, 300),\n",
       " (7, 5, 325),\n",
       " (7, 5, 350),\n",
       " (7, 5, 375),\n",
       " (7, 6, 50),\n",
       " (7, 6, 75),\n",
       " (7, 6, 100),\n",
       " (7, 6, 125),\n",
       " (7, 6, 150),\n",
       " (7, 6, 175),\n",
       " (7, 6, 200),\n",
       " (7, 6, 225),\n",
       " (7, 6, 250),\n",
       " (7, 6, 275),\n",
       " (7, 6, 300),\n",
       " (7, 6, 325),\n",
       " (7, 6, 350),\n",
       " (7, 6, 375),\n",
       " (7, 7, 50),\n",
       " (7, 7, 75),\n",
       " (7, 7, 100),\n",
       " (7, 7, 125),\n",
       " (7, 7, 150),\n",
       " (7, 7, 175),\n",
       " (7, 7, 200),\n",
       " (7, 7, 225),\n",
       " (7, 7, 250),\n",
       " (7, 7, 275),\n",
       " (7, 7, 300),\n",
       " (7, 7, 325),\n",
       " (7, 7, 350),\n",
       " (7, 7, 375),\n",
       " (7, 8, 50),\n",
       " (7, 8, 75),\n",
       " (7, 8, 100),\n",
       " (7, 8, 125),\n",
       " (7, 8, 150),\n",
       " (7, 8, 175),\n",
       " (7, 8, 200),\n",
       " (7, 8, 225),\n",
       " (7, 8, 250),\n",
       " (7, 8, 275),\n",
       " (7, 8, 300),\n",
       " (7, 8, 325),\n",
       " (7, 8, 350),\n",
       " (7, 8, 375),\n",
       " (7, 9, 50),\n",
       " (7, 9, 75),\n",
       " (7, 9, 100),\n",
       " (7, 9, 125),\n",
       " (7, 9, 150),\n",
       " (7, 9, 175),\n",
       " (7, 9, 200),\n",
       " (7, 9, 225),\n",
       " (7, 9, 250),\n",
       " (7, 9, 275),\n",
       " (7, 9, 300),\n",
       " (7, 9, 325),\n",
       " (7, 9, 350),\n",
       " (7, 9, 375),\n",
       " (7, 10, 50),\n",
       " (7, 10, 75),\n",
       " (7, 10, 100),\n",
       " (7, 10, 125),\n",
       " (7, 10, 150),\n",
       " (7, 10, 175),\n",
       " (7, 10, 200),\n",
       " (7, 10, 225),\n",
       " (7, 10, 250),\n",
       " (7, 10, 275),\n",
       " (7, 10, 300),\n",
       " (7, 10, 325),\n",
       " (7, 10, 350),\n",
       " (7, 10, 375),\n",
       " (7, 11, 50),\n",
       " (7, 11, 75),\n",
       " (7, 11, 100),\n",
       " (7, 11, 125),\n",
       " (7, 11, 150),\n",
       " (7, 11, 175),\n",
       " (7, 11, 200),\n",
       " (7, 11, 225),\n",
       " (7, 11, 250),\n",
       " (7, 11, 275),\n",
       " (7, 11, 300),\n",
       " (7, 11, 325),\n",
       " (7, 11, 350),\n",
       " (7, 11, 375),\n",
       " (7, 12, 50),\n",
       " (7, 12, 75),\n",
       " (7, 12, 100),\n",
       " (7, 12, 125),\n",
       " (7, 12, 150),\n",
       " (7, 12, 175),\n",
       " (7, 12, 200),\n",
       " (7, 12, 225),\n",
       " (7, 12, 250),\n",
       " (7, 12, 275),\n",
       " (7, 12, 300),\n",
       " (7, 12, 325),\n",
       " (7, 12, 350),\n",
       " (7, 12, 375),\n",
       " (7, 13, 50),\n",
       " (7, 13, 75),\n",
       " (7, 13, 100),\n",
       " (7, 13, 125),\n",
       " (7, 13, 150),\n",
       " (7, 13, 175),\n",
       " (7, 13, 200),\n",
       " (7, 13, 225),\n",
       " (7, 13, 250),\n",
       " (7, 13, 275),\n",
       " (7, 13, 300),\n",
       " (7, 13, 325),\n",
       " (7, 13, 350),\n",
       " (7, 13, 375),\n",
       " (7, 14, 50),\n",
       " (7, 14, 75),\n",
       " (7, 14, 100),\n",
       " (7, 14, 125),\n",
       " (7, 14, 150),\n",
       " (7, 14, 175),\n",
       " (7, 14, 200),\n",
       " (7, 14, 225),\n",
       " (7, 14, 250),\n",
       " (7, 14, 275),\n",
       " (7, 14, 300),\n",
       " (7, 14, 325),\n",
       " (7, 14, 350),\n",
       " (7, 14, 375),\n",
       " (8, 2, 50),\n",
       " (8, 2, 75),\n",
       " (8, 2, 100),\n",
       " (8, 2, 125),\n",
       " (8, 2, 150),\n",
       " (8, 2, 175),\n",
       " (8, 2, 200),\n",
       " (8, 2, 225),\n",
       " (8, 2, 250),\n",
       " (8, 2, 275),\n",
       " (8, 2, 300),\n",
       " (8, 2, 325),\n",
       " (8, 2, 350),\n",
       " (8, 2, 375),\n",
       " (8, 3, 50),\n",
       " (8, 3, 75),\n",
       " (8, 3, 100),\n",
       " (8, 3, 125),\n",
       " (8, 3, 150),\n",
       " (8, 3, 175),\n",
       " (8, 3, 200),\n",
       " (8, 3, 225),\n",
       " (8, 3, 250),\n",
       " (8, 3, 275),\n",
       " (8, 3, 300),\n",
       " (8, 3, 325),\n",
       " (8, 3, 350),\n",
       " (8, 3, 375),\n",
       " (8, 4, 50),\n",
       " (8, 4, 75),\n",
       " (8, 4, 100),\n",
       " (8, 4, 125),\n",
       " (8, 4, 150),\n",
       " (8, 4, 175),\n",
       " (8, 4, 200),\n",
       " (8, 4, 225),\n",
       " (8, 4, 250),\n",
       " (8, 4, 275),\n",
       " (8, 4, 300),\n",
       " (8, 4, 325),\n",
       " (8, 4, 350),\n",
       " (8, 4, 375),\n",
       " (8, 5, 50),\n",
       " (8, 5, 75),\n",
       " (8, 5, 100),\n",
       " (8, 5, 125),\n",
       " (8, 5, 150),\n",
       " (8, 5, 175),\n",
       " (8, 5, 200),\n",
       " (8, 5, 225),\n",
       " (8, 5, 250),\n",
       " (8, 5, 275),\n",
       " (8, 5, 300),\n",
       " (8, 5, 325),\n",
       " (8, 5, 350),\n",
       " (8, 5, 375),\n",
       " (8, 6, 50),\n",
       " (8, 6, 75),\n",
       " (8, 6, 100),\n",
       " (8, 6, 125),\n",
       " (8, 6, 150),\n",
       " (8, 6, 175),\n",
       " (8, 6, 200),\n",
       " (8, 6, 225),\n",
       " (8, 6, 250),\n",
       " (8, 6, 275),\n",
       " (8, 6, 300),\n",
       " (8, 6, 325),\n",
       " (8, 6, 350),\n",
       " (8, 6, 375),\n",
       " (8, 7, 50),\n",
       " (8, 7, 75),\n",
       " (8, 7, 100),\n",
       " (8, 7, 125),\n",
       " (8, 7, 150),\n",
       " (8, 7, 175),\n",
       " (8, 7, 200),\n",
       " (8, 7, 225),\n",
       " (8, 7, 250),\n",
       " (8, 7, 275),\n",
       " (8, 7, 300),\n",
       " (8, 7, 325),\n",
       " (8, 7, 350),\n",
       " (8, 7, 375),\n",
       " (8, 8, 50),\n",
       " (8, 8, 75),\n",
       " (8, 8, 100),\n",
       " (8, 8, 125),\n",
       " (8, 8, 150),\n",
       " (8, 8, 175),\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
